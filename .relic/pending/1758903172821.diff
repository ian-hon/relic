= LOCAL 1758903172821 "" "" no_one
+ D . src
+ F .%2Fsrc error.rs
+ D .%2Fsrc commands
+ F .%2Fsrc%2Fcommands pending.rs
+ F .%2Fsrc%2Fcommands tree.rs
+ F .%2Fsrc%2Fcommands clone.rs
+ F .%2Fsrc%2Fcommands staging.rs
+ F .%2Fsrc%2Fcommands push.rs
+ F .%2Fsrc%2Fcommands remove.rs
+ F .%2Fsrc%2Fcommands stash.rs
+ F .%2Fsrc%2Fcommands qhar.rs
+ F .%2Fsrc%2Fcommands commit.rs
+ F .%2Fsrc%2Fcommands cherry.rs
+ F .%2Fsrc%2Fcommands pull.rs
+ F .%2Fsrc%2Fcommands detach.rs
+ F .%2Fsrc%2Fcommands fetch.rs
+ F .%2Fsrc%2Fcommands init.rs
+ F .%2Fsrc%2Fcommands mod.rs
+ F .%2Fsrc%2Fcommands rollback.rs
+ F .%2Fsrc%2Fcommands test.rs
+ F .%2Fsrc%2Fcommands add.rs
+ F .%2Fsrc%2Fcommands branch.rs
+ D .%2Fsrc core
+ F .%2Fsrc%2Fcore relic_info.rs
+ F .%2Fsrc%2Fcore state.rs
+ F .%2Fsrc%2Fcore paths.rs
+ F .%2Fsrc%2Fcore relic.rs
+ F .%2Fsrc%2Fcore commit.rs
+ D .%2Fsrc%2Fcore objects
+ F .%2Fsrc%2Fcore%2Fobjects mod.rs
+ D .%2Fsrc%2Fcore%2Fobjects data
+ F .%2Fsrc%2Fcore%2Fobjects%2Fdata blob.rs
+ F .%2Fsrc%2Fcore%2Fobjects%2Fdata upstream.rs
+ F .%2Fsrc%2Fcore%2Fobjects%2Fdata tree.rs
+ F .%2Fsrc%2Fcore%2Fobjects%2Fdata content_set.rs
+ F .%2Fsrc%2Fcore%2Fobjects%2Fdata mod.rs
+ F .%2Fsrc%2Fcore%2Fobjects%2Fdata content.rs
+ D .%2Fsrc%2Fcore%2Fobjects modifications
+ D .%2Fsrc%2Fcore%2Fobjects%2Fmodifications change
+ F .%2Fsrc%2Fcore%2Fobjects%2Fmodifications%2Fchange serialisation.rs
+ F .%2Fsrc%2Fcore%2Fobjects%2Fmodifications%2Fchange mod.rs
+ F .%2Fsrc%2Fcore%2Fobjects%2Fmodifications%2Fchange filter.rs
+ F .%2Fsrc%2Fcore%2Fobjects%2Fmodifications%2Fchange constructor.rs
+ F .%2Fsrc%2Fcore%2Fobjects%2Fmodifications%2Fchange inverse.rs
+ F .%2Fsrc%2Fcore%2Fobjects%2Fmodifications blob.rs
+ F .%2Fsrc%2Fcore%2Fobjects%2Fmodifications mod.rs
+ F .%2Fsrc%2Fcore%2Fobjects%2Fmodifications container.rs
+ F .%2Fsrc%2Fcore mod.rs
+ F .%2Fsrc utils.rs
+ F .%2Fsrc cli.rs
+ F .%2Fsrc main.rs
+ F .%2Fsrc lib.rs
+ D . lorem
+ F .%2Florem earth
+ F .%2Florem mars
+ D .%2Florem ipsum
+ F .%2Florem%2Fipsum temp
+ F .%2Florem%2Fipsum saturn
+ D .%2Florem%2Fipsum dolor
+ F .%2Florem%2Fipsum%2Fdolor pluto
+ F .%2Florem pluto
+ F . .relic_ignore
+ F . Cargo.toml
+ F . ipsum
=
| . .relic_ignore
+ 0 "-- Added by Relic: Automatically ignore all git content"
+ 1 ".git/"
+ 2 ".gitignore"
+ 4 "target/"
+ 5 "Cargo.lock"
| . Cargo.toml
+ 0 "[package]"
+ 1 "name = \"relic-vcs\""
+ 2 "version = \"0.1.0\""
+ 3 "edition = \"2021\""
+ 4 "exclude = ["
+ 5 "    \"lorem/\","
+ 6 "    \"ipsum\","
+ 7 "    \".relic\","
+ 8 "    \".relic_ignore\","
+ 9 "    \"target/\""
+ 10 "]"
+ 11 "license = \"MIT\""
+ 12 "keywords = [\"vcs\"]"
+ 13 "categories = [\"command-line-interface\", \"filesystem\"]"
+ 14 "description = \"Git, but in Rust.\""
+ 15 "homepage = \"https://github.com/ian-hon/relic\""
+ 16 "repository = \"https://github.com/ian-hon/relic\""
+ 17 "readme = false"
+ 19 "[dependencies]"
+ 20 "sha256 = \"1.5.0\""
+ 21 ""
+ 22 "strum = \"0.26.3\""
+ 23 "strum_macros = \"0.26.3\""
+ 24 ""
+ 25 "serde = { version = \"1.0\", features = [\"derive\"] }"
+ 26 "serde_json = \"1.0\""
+ 27 "urlencoding = \"2.1.3\""
+ 28 "unescape = \"0.1.0\""
+ 29 "chrono = \"0.4.41\""
+ 30 ""
+ 31 "dotenv = \"0.15.0\""
+ 32 "clap = \"4.5.38\""
+ 33 ""
+ 34 "similar = \"2.7.0\""
+ 35 "# diff = \"0.1.13\""
| . ipsum
+ 0 ",\\n};\\n\\n#[derive(Debug, Clone)]\\npub struct Commit {\\n    pub id: Option<u32>,\\n    pub message: String,\\n    pub description: String,\\n    pub change: Change,\\n    pub timestamp: u64,\\n\\n    pub author: String,\\n}\\nimpl Commit {\\n    pub fn header(&self) -> String {\\n        // \\\"integrated backwards compatibility\\\" (2025-5-26 16:30) (affected : change.rs, content.rs, ...)\\n\\n        let mut file_names = vec![];\\n        for (_, parent) in self.change.as_map().1 {\\n            for (f, _) in parent {\\n                file_names.push(f);\\n            }\\n        }\\n\\n        format!(\\n            \\\"({}) \\\\\\\"{}\\\\\\\" (affected : {}{})\\\",\\n            utils::into_human_readable(self.timestamp),\\n            self.message,\\n            file_names\\n                .iter()\\n                .take(5)\\n                .map(|x| x.to_string())\\n                .collect::<Vec<String>>()\\n                .join(\\\", \\\"),\\n            if file_names.len() > 5 { \\\", ...\\\" } else { \\\"\\\" }\\n        )\\n    }\\n\\n    pub fn serialise(&self) -> String {\\n        format!(\\n            \\\"= {} {} {:?} {:?} {}\\\\n{}\\\",\\n            self.id\\n                .map_or(\\\"LOCAL\\\".to_string(), |i| format!(\\\"{:06x}\\\", i).clone()),\\n            self.timestamp,\\n            urlencoding::encode(&self.message).to_string(),\\n            urlencoding::encode(&self.description).to_string(),\\n            self.author,\\n            self.change.serialise_changes()\\n        )\\n    }\\n\\n    pub fn deserialise(s: String) -> Option<Commit> {\\n        // = LOCAL 1747682692319414000 \\\"initial%20commit\\\" \\\"\\\" no_one\\n\\n        let lines = s.split(\\\"\\\\n\\\").collect::<Vec<&str>>();\\n        if lines.len() < 2 {\\n            // return None;\\n        }\\n\\n        let metadata = lines[0].split(\\\" \\\").collect::<Vec<&str>>();\\n        if metadata.len() != 6 {\\n            // return None;\\n        }\\n\\n        let [_, status, time, message, description, author] = *metadata.as_slice() else {\\n            return None;\\n        };\\n\\n        Some(Commit {\\n            id: status.parse::<u32>().map_or(None, |t| Some(t)),\\n            message: urlencoding::decode(&message[1..message.len() - 1].to_string())\\n                .unwrap()\\n                .to_string(),\\n            description: urlencoding::decode(&description[1..description.len() - 1].to_string())\\n                .unwrap()\\n                .to_string(),\\n            change: Change::deserialise_changes(lines[1..].join(\\\"\\\\n\\\")).unwrap_or(Change::empty()),\\n            timestamp: time.parse::<u64>().unwrap_or(0),\\n            author: author.to_string(),\\n        })\\n    }\\n}\\n\\npub fn add(_: &mut State, args: &ArgMatches) {\\n    let f = args\\n        .get_many::<PathBuf>(\\\"FILE\\\")\\n        .unwrap()\\n        .map(|x| x.clone())\\n        .collect::<Vec<PathBuf>>();\\n\\n    let mut result: HashSet<String> = HashSet::from_iter(\\n        fs::read_to_string(\\\"./.relic/tracked\\\")\\n            .unwrap()\\n            .split(\\\"\\\\n\\\")\\n            .filter(|x| !x.is_empty())\\n            .map(|x| x.to_string())\\n            .collect::<Vec<String>>(),\\n    );\\n    for p in f {\\n        // TODO : path.join for this? or concatenating / works?\\n        result.insert(format!(\\n            \\\"{}{}\\\",\\n            p.to_string_lossy().to_string(),\\n            if !p.to_string_lossy().to_string().ends_with(\\\"/\\\") && p.is_dir() {\\n                \\\"/\\\"\\n            } else {\\n                \\\"\\\"\\n            }\\n        ));\\n    }\\n    let _ = fs::write(\\n        \\\"./.relic/tracked\\\",\\n        result.drain().collect::<Vec<String>>().join(\\\"\\\\n\\\"),\\n    );\\n}\\n\\npub fn remove(s: &mut State, args: &ArgMatches) {\\n    let f = args\\n        .get_many::<PathBuf>(\\\"FILE\\\")\\n        .unwrap()\\n        .map(|x| x.clone())\\n        .collect::<Vec<PathBuf>>();\\n\\n    let result: HashSet<String> = HashSet::from_iter(\\n        fs::read_to_string(\\\"./.relic/tracked\\\")\\n            .unwrap()\\n            .split(\\\"\\\\n\\\")\\n            .filter(|x| !x.is_empty())\\n            .map(|x| PathBuf::from(\\\".\\\").join(x).to_string_lossy().to_string())\\n            .collect::<Vec<String>>(),\\n    );\\n\\n    // initialise removed_content\\n    let mut removed_content = ContentSet {\\n        files: HashSet::from_iter(\\n            f.iter()\\n                .filter(|x| !x.is_dir())\\n                .map(|x| PathBuf::from(\\\".\\\").join(x).to_string_lossy().to_string()),\\n        ),\\n        directories: HashSet::from_iter(\\n            f.iter()\\n                .filter(|x| x.is_dir())\\n                .map(|x| PathBuf::from(\\\".\\\").join(x).to_string_lossy().to_string()),\\n        ),\\n    }\\n    .initialise(&mut s.current);\\n\\n    let mut to_subtract: HashSet<String> = HashSet::from_iter(\\n        removed_content\\n            .directories\\n            .drain()\\n            .collect::<Vec<String>>()\\n            .into_iter()\\n            .map(|x| format!(\\\"{x}/\\\"))\\n            .collect::<Vec<String>>(),\\n    );\\n    to_subtract = to_subtract\\n        .union(&HashSet::from_iter(removed_content.files.drain()))\\n        .map(|x| x.to_string())\\n        .collect::<HashSet<String>>();\\n\\n    // set operations\\n    // right join\\n    // result - removed_content\\n\\n    let _ = fs::write(\\n        \\\"./.relic/tracked\\\",\\n        result\\n            .difference(&to_subtract)\\n            .map(|x| x[2..].to_string())\\n            .collect::<Vec<String>>()\\n            .join(\\\"\\\\n\\\"),\\n    );\\n}\\n\\npub fn commit(state: &mut State, args: &ArgMatches) {\\n    // push into pending stage\\n    // update upstream\\n\\n    // everything after the first line will be generated by Change::serialise_change\\n    r#\\\"= {commit id} {unix timestamp of commit} {message} {description} {author}\\n+ D \\\"lorem/ipsum/dolor\\\"\\n+ F \\\"lorem/ipsum/dolor/earth.txt\\\" \\\"earth.txt\\\"\\n- D \\\"lorem/sit\\\"\\n=\\n| \\\"lorem/ipsum/dolor/earth.txt\\\"\\n+ 3 asdfsdf\\n+ 5 sfsdf\\n- 7\\n| \\\"lorem/ipsum/saturn/txt\\\"\\n+ 4 lsdfljs\\\"#;\\n    let message = args.get_one::<String>(\\\"message\\\").unwrap().clone();\\n    let description = args\\n        .get_one::<String>(\\\"description\\\")\\n        .map_or(\\\"\\\".to_string(), String::clone);\\n\\n    let commit = Commit {\\n        id: None,\\n        message,\\n        description,\\n        change: state.get_changes(),\\n        timestamp: utils::get_time(),\\n        author: \\\"no_one\\\".to_string(),\\n    };\\n\\n    state.pending_add(commit);\\n    // update upstream\\n    (*state).update_upstream(&mut state.track_set.clone());\\n}\\n\\npub fn push(_: &mut State, _: &ArgMatches) {}\\n\\npub fn pull(_: &mut State, _: &ArgMatches) {}\\n\\npub fn fetch(_: &mut State, _: &ArgMatches) {}\\n\\npub fn cherry(_: &mut State, _: &ArgMatches) {}\\n\\npub fn rollback(_: &mut State, _: &ArgMatches) {}\\n\\npub fn pending(state: &mut State, args: &ArgMatches) {\\n    let pending = state.pending_get();\\n\\n    if let Some(commit_number) = args\\n        .get_one::<String>(\\\"COMMIT\\\")\\n        .map_or(None, |x| x.parse::<i32>().map_or(None, |x| Some(x)))\\n    {\\n        // display selected\\n        if (commit_number < 0) || (commit_number >= pending.len() as i32) {\\n            println!(\\n                \\\"Invalid selection. Please select commit numbers in the range of (0-{})\\\",\\n                pending.len() - 1\\n            );\\n            return;\\n        }\\n\\n        let copy = state.current.clone();\\n        let changes = pending[commit_number as usize].clone();\\n        println!(\\\"before : {}\\\", copy.get_hash());\\n\\n        let mut inversed = copy.clone();\\n        inversed.unapply_changes(changes.change.clone());\\n        println!(\\\"inverse : {}\\\", inversed.get_hash());\\n\\n        let mut after = inversed.clone();\\n        after.apply_changes(changes.change.clone());\\n        println!(\\\"after : {}\\\", after.get_hash());\\n\\n        let mut a = after\\n            .serialise()\\n            .split(\\\"\\\\n\\\")\\n            .map(|x| x.to_string())\\n            .collect::<Vec<String>>();\\n        let mut b = copy\\n            .serialise()\\n            .split(\\\"\\\\n\\\")\\n            .map(|x| x.to_string())\\n            .collect::<Vec<String>>();\\n\\n        a.sort();\\n        b.sort();\\n\\n        // println!(\\\"{}\\\", utils::generate_tree(&copy));\\n        // println!(\\\"{}\\\", utils::generate_tree(&after));\\n        // println!(\\\"{}\\\", utils::generate_tree(&inversed));\\n\\n        for i in 0..a.len() {\\n            let c = a[i].clone();\\n            let d = b[i].clone();\\n            if c != d {\\n                println!(\\\"{c}\\\\n{d}\\\\n\\\\n\\\\n\\\\n\\\\n\\\");\\n            }\\n        }\\n    } else {\\n        // display all\\n        for (index, c) in pending.iter().enumerate() {\\n            println!(\\\"{index}. {}\\\", c.header());\\n        }\\n    }\\n}\\n\""
+ 1 ",\\n};\\n\\n#[derive(Debug, Clone)]\\npub struct Commit {\\n    pub id: Option<u32>,\\n    pub message: String,\\n    pub description: String,\\n    pub change: Change,\\n    pub timestamp: u64,\\n\\n    pub author: String,\\n}\\nimpl Commit {\\n    pub fn header(&self) -> String {\\n        // \\\"integrated backwards compatibility\\\" (2025-5-26 16:30) (affected : change.rs, content.rs, ...)\\n\\n        let mut file_names = vec![];\\n        for (_, parent) in self.change.as_map().1 {\\n            for (f, _) in parent {\\n                file_names.push(f);\\n            }\\n        }\\n\\n        format!(\\n            \\\"({}) \\\\\\\"{}\\\\\\\" (affected : {}{})\\\",\\n            utils::into_human_readable(self.timestamp),\\n            self.message,\\n            file_names\\n                .iter()\\n                .take(5)\\n                .map(|x| x.to_string())\\n                .collect::<Vec<String>>()\\n                .join(\\\", \\\"),\\n            if file_names.len() > 5 { \\\", ...\\\" } else { \\\"\\\" }\\n        )\\n    }\\n\\n    pub fn serialise(&self) -> String {\\n        format!(\\n            \\\"= {} {} {:?} {:?} {}\\\\n{}\\\",\\n            self.id\\n                .map_or(\\\"LOCAL\\\".to_string(), |i| format!(\\\"{:06x}\\\", i).clone()),\\n            self.timestamp,\\n            urlencoding::encode(&self.message).to_string(),\\n            urlencoding::encode(&self.description).to_string(),\\n            self.author,\\n            self.change.serialise_changes()\\n        )\\n    }\\n\\n    pub fn deserialise(s: String) -> Option<Commit> {\\n        // = LOCAL 1747682692319414000 \\\"initial%20commit\\\" \\\"\\\" no_one\\n\\n        let lines = s.split(\\\"\\\\n\\\").collect::<Vec<&str>>();\\n        if lines.len() < 2 {\\n            // return None;\\n        }\\n\\n        let metadata = lines[0].split(\\\" \\\").collect::<Vec<&str>>();\\n        if metadata.len() != 6 {\\n            // return None;\\n        }\\n\\n        let [_, status, time, message, description, author] = *metadata.as_slice() else {\\n            return None;\\n        };\\n\\n        Some(Commit {\\n            id: status.parse::<u32>().map_or(None, |t| Some(t)),\\n            message: urlencoding::decode(&message[1..message.len() - 1].to_string())\\n                .unwrap()\\n                .to_string(),\\n            description: urlencoding::decode(&description[1..description.len() - 1].to_string())\\n                .unwrap()\\n                .to_string(),\\n            change: Change::deserialise_changes(lines[1..].join(\\\"\\\\n\\\")).unwrap_or(Change::empty()),\\n            timestamp: time.parse::<u64>().unwrap_or(0),\\n            author: author.to_string(),\\n        })\\n    }\\n}\\n\\npub fn add(_: &mut State, args: &ArgMatches) {\\n    let f = args\\n        .get_many::<PathBuf>(\\\"FILE\\\")\\n        .unwrap()\\n        .map(|x| x.clone())\\n        .collect::<Vec<PathBuf>>();\\n\\n    let mut result: HashSet<String> = HashSet::from_iter(\\n        fs::read_to_string(\\\"./.relic/tracked\\\")\\n            .unwrap()\\n            .split(\\\"\\\\n\\\")\\n            .filter(|x| !x.is_empty())\\n            .map(|x| x.to_string())\\n            .collect::<Vec<String>>(),\\n    );\\n    for p in f {\\n        // TODO : path.join for this? or concatenating / works?\\n        result.insert(format!(\\n            \\\"{}{}\\\",\\n            p.to_string_lossy().to_string(),\\n            if !p.to_string_lossy().to_string().ends_with(\\\"/\\\") && p.is_dir() {\\n                \\\"/\\\"\\n            } else {\\n                \\\"\\\"\\n            }\\n        ));\\n    }\\n    let _ = fs::write(\\n        \\\"./.relic/tracked\\\",\\n        result.drain().collect::<Vec<String>>().join(\\\"\\\\n\\\"),\\n    );\\n}\\n\\npub fn remove(s: &mut State, args: &ArgMatches) {\\n    let f = args\\n        .get_many::<PathBuf>(\\\"FILE\\\")\\n        .unwrap()\\n        .map(|x| x.clone())\\n        .collect::<Vec<PathBuf>>();\\n\\n    let result: HashSet<String> = HashSet::from_iter(\\n        fs::read_to_string(\\\"./.relic/tracked\\\")\\n            .unwrap()\\n            .split(\\\"\\\\n\\\")\\n            .filter(|x| !x.is_empty())\\n            .map(|x| PathBuf::from(\\\".\\\").join(x).to_string_lossy().to_string())\\n            .collect::<Vec<String>>(),\\n    );\\n\\n    // initialise removed_content\\n    let mut removed_content = ContentSet {\\n        files: HashSet::from_iter(\\n            f.iter()\\n                .filter(|x| !x.is_dir())\\n                .map(|x| PathBuf::from(\\\".\\\").join(x).to_string_lossy().to_string()),\\n        ),\\n        directories: HashSet::from_iter(\\n            f.iter()\\n                .filter(|x| x.is_dir())\\n                .map(|x| PathBuf::from(\\\".\\\").join(x).to_string_lossy().to_string()),\\n        ),\\n    }\\n    .initialise(&mut s.current);\\n\\n    let mut to_subtract: HashSet<String> = HashSet::from_iter(\\n        removed_content\\n            .directories\\n            .drain()\\n            .collect::<Vec<String>>()\\n            .into_iter()\\n            .map(|x| format!(\\\"{x}/\\\"))\\n            .collect::<Vec<String>>(),\\n    );\\n    to_subtract = to_subtract\\n        .union(&HashSet::from_iter(removed_content.files.drain()))\\n        .map(|x| x.to_string())\\n        .collect::<HashSet<String>>();\\n\\n    // set operations\\n    // right join\\n    // result - removed_content\\n\\n    let _ = fs::write(\\n        \\\"./.relic/tracked\\\",\\n        result\\n            .difference(&to_subtract)\\n            .map(|x| x[2..].to_string())\\n            .collect::<Vec<String>>()\\n            .join(\\\"\\\\n\\\"),\\n    );\\n}\\n\\npub fn commit(state: &mut State, args: &ArgMatches) {\\n    // push into pending stage\\n    // update upstream\\n\\n    // everything after the first line will be generated by Change::serialise_change\\n    r#\\\"= {commit id} {unix timestamp of commit} {message} {description} {author}\\n+ D \\\"lorem/ipsum/dolor\\\"\\n+ F \\\"lorem/ipsum/dolor/earth.txt\\\" \\\"earth.txt\\\"\\n- D \\\"lorem/sit\\\"\\n=\\n| \\\"lorem/ipsum/dolor/earth.txt\\\"\\n+ 3 asdfsdf\\n+ 5 sfsdf\\n- 7\\n| \\\"lorem/ipsum/saturn/txt\\\"\\n+ 4 lsdfljs\\\"#;\\n    let message = args.get_one::<String>(\\\"message\\\").unwrap().clone();\\n    let description = args\\n        .get_one::<String>(\\\"description\\\")\\n        .map_or(\\\"\\\".to_string(), String::clone);\\n\\n    let commit = Commit {\\n        id: None,\\n        message,\\n        description,\\n        change: state.get_changes(),\\n        timestamp: utils::get_time(),\\n        author: \\\"no_one\\\".to_string(),\\n    };\\n\\n    state.pending_add(commit);\\n    // update upstream\\n    (*state).update_upstream(&mut state.track_set.clone());\\n}\\n\\npub fn push(_: &mut State, _: &ArgMatches) {}\\n\\npub fn pull(_: &mut State, _: &ArgMatches) {}\\n\\npub fn fetch(_: &mut State, _: &ArgMatches) {}\\n\\npub fn cherry(_: &mut State, _: &ArgMatches) {}\\n\\npub fn rollback(_: &mut State, _: &ArgMatches) {}\\n\\npub fn pending(state: &mut State, args: &ArgMatches) {\\n    let pending = state.pending_get();\\n\\n    if let Some(commit_number) = args\\n        .get_one::<String>(\\\"COMMIT\\\")\\n        .map_or(None, |x| x.parse::<i32>().map_or(None, |x| Some(x)))\\n    {\\n        // display selected\\n        if (commit_number < 0) || (commit_number >= pending.len() as i32) {\\n            println!(\\n                \\\"Invalid selection. Please select commit numbers in the range of (0-{})\\\",\\n                pending.len() - 1\\n            );\\n            return;\\n        }\\n\\n        let copy = state.current.clone();\\n        let changes = pending[commit_number as usize].clone();\\n        println!(\\\"before : {}\\\", copy.get_hash());\\n\\n        let mut after = copy.clone();\\n        after.apply_changes(changes.change.clone());\\n        println!(\\\"after : {}\\\", after.get_hash());\\n\\n        let mut inversed = after.clone();\\n        inversed.unapply_changes(changes.change.clone());\\n        println!(\\\"inverse : {}\\\", inversed.get_hash());\\n\\n        let mut a = inversed\\n            .serialise()\\n            .split(\\\"\\\\n\\\")\\n            .map(|x| x.to_string())\\n            .collect::<Vec<String>>();\\n        let mut b = copy\\n            .serialise()\\n            .split(\\\"\\\\n\\\")\\n            .map(|x| x.to_string())\\n            .collect::<Vec<String>>();\\n\\n        a.sort();\\n        b.sort();\\n\\n        // println!(\\\"{}\\\", utils::generate_tree(&copy));\\n        // println!(\\\"{}\\\", utils::generate_tree(&after));\\n        // println!(\\\"{}\\\", utils::generate_tree(&inversed));\\n\\n        for i in 0..a.len() {\\n            let c = a[i].clone();\\n            let d = b[i].clone();\\n            if c != d {\\n                println!(\\\"{c}\\\\n{d}\\\\n\\\\n\\\\n\\\\n\\\\n\\\");\\n            }\\n        }\\n        // display all\\n        for (index, c) in pending.iter().enumerate() {\\n            println!(\\\"{index}. {}\\\", c.header());\\n        }\\n    }\\n}\\n\""
| .%2Florem earth
- 0 ""
+ 0 "can you understand me?"
| .%2Florem mars
- 0 ""
+ 0 "sldfjsljdkf"
| .%2Florem%2Fipsum temp
- 0 ""
+ 0 "alsfdk"
| .%2Fsrc cli.rs
+ 0 "use std::collections::HashMap;"
+ 1 "use std::path::PathBuf;"
+ 2 ""
+ 3 "use clap::{arg, value_parser, ArgMatches, Command};"
+ 4 ""
+ 5 "use crate::commands as command_module;"
+ 6 "use crate::core::state::State;"
+ 7 "use crate::error::RelicError;"
+ 8 ""
+ 9 "// add"
+ 10 "// commit {message}"
+ 11 "// push"
+ 12 "// pull"
+ 13 "// fetch"
+ 14 "// branch {name}"
+ 15 "//      will change to that branch"
+ 16 "//      if branch doesnt exist, create"
+ 17 "//      ask to create stash (if changes present)"
+ 18 "// stash {name|optional}"
+ 19 "//      stashes are bound to a branch"
+ 20 "//      optional to have a name"
+ 21 "// restore"
+ 22 "//      select stash to restore"
+ 23 "// rollback"
+ 24 "//      resets to current head"
+ 25 "// cherry {commit hash}"
+ 26 ""
+ 27 "pub type CommandType = fn(&mut State, &ArgMatches);"
+ 28 ""
+ 29 "pub struct CommandHandler {"
+ 30 "    commands: HashMap<String, CommandType>,"
+ 31 "    pub handler: Command,"
+ 32 "}"
+ 33 ""
+ 34 "pub fn build() -> CommandHandler {"
+ 35 "    let mut command_handler = Command::new(\"relic\")"
+ 36 "        .about("
+ 37 "            r#\"This is the Relic Version Control System."
+ 38 ""
+ 39 "The best way to learn is to stupidly and"
+ 40 "blindly reinvent the wheel."
+ 41 ""
+ 42 "Relic is a simple hobby project, because"
+ 43 "remaking Git sounded fun and interesting."
+ 44 ""
+ 45 "Most common features like committing,"
+ 46 "pushing and pulling, are implemented.\"#,"
+ 47 "        )"
+ 48 "        .subcommand_required(true)"
+ 49 "        .arg_required_else_help(true);"
+ 50 ""
+ 51 "    type CommandType = fn(&mut State, &ArgMatches);"
+ 52 "    let mut commands: HashMap<String, CommandType> = HashMap::new();"
+ 53 "    for (f, c) in HashMap::<CommandType, clap::Command>::from_iter::<"
+ 54 "        Vec<(CommandType, clap::Command)>,"
+ 55 "    >(vec!["
+ 56 "        ("
+ 57 "            command_module::init,"
+ 58 "            Command::new(\"init\").about(\"Initialises a Relic repository in the current directory.\"),"
+ 59 "        ),"
+ 60 "        ("
+ 61 "            command_module::clone,"
+ 62 "            Command::new(\"clone\").about(\"Clone a remote Relic repository in the current directory.\")"
+ 63 "            .arg_required_else_help(true)"
+ 64 "            .arg("
+ 65 "                arg!([URL] \"URL of the remote Relic repository\")"
+ 66 "                .required(true)"
+ 67 "            )"
+ 68 "        ),"
+ 69 "        ("
+ 70 "            command_module::detach,"
+ 71 "            Command::new(\"detach\").about(\"Completely removes Relic from the current directory.\")"
+ 72 "        ),"
+ 73 "        ("
+ 74 "            command_module::add,"
+ 75 "            Command::new(\"add\")"
+ 76 "                .about(\"Adds a file(s) to staging\")"
+ 77 "                .arg_required_else_help(true)"
+ 78 "                .arg("
+ 79 "                    arg!([FILE]... \"File(s) to add (* for all)\")"
+ 80 "                        .required(true)"
+ 81 "                        .value_parser(value_parser!(PathBuf)),"
+ 82 "                ),"
+ 83 "        ),"
+ 84 "        ("
+ 85 "            command_module::remove,"
+ 86 "            Command::new(\"remove\")"
+ 87 "                .about(\"Removes a file(s) to staging\")"
+ 88 "                .arg_required_else_help(true)"
+ 89 "                .arg("
+ 90 "                    arg!([FILE]... \"File(s) to remove (* for all)\")"
+ 91 "                        .required(true)"
+ 92 "                        .value_parser(value_parser!(PathBuf)),"
+ 93 "                ),"
+ 94 "        ),"
+ 95 "        ("
+ 96 "            command_module::commit,"
+ 97 "            Command::new(\"commit\")"
+ 98 "                .about(\"Commit current changes.\")"
+ 99 "                .arg_required_else_help(true)"
+ 100 "                .arg(arg!(-m --message <MESSAGE> \"Commit message\").required(true))"
+ 101 "                .arg(arg!(-d --description <DESCRIPTION> \"Commit description\")),"
+ 102 "        ),"
+ 103 "        ("
+ 104 "            command_module::push,"
+ 105 "            Command::new(\"push\").about(\"Pushes local changes to remote.\"),"
+ 106 "        ),"
+ 107 "        ("
+ 108 "            command_module::pull,"
+ 109 "            Command::new(\"pull\").about(\"Pull changes from remote to local.\"),"
+ 110 "        ),"
+ 111 "        ("
+ 112 "            command_module::fetch,"
+ 113 "            Command::new(\"fetch\").about(\"Check remote for new changes.\"),"
+ 114 "        ),"
+ 115 "        ("
+ 116 "            command_module::branch,"
+ 117 "            Command::new(\"branch\").about(\"\")"
+ 118 "        ),"
+ 119 "        ("
+ 120 "            command_module::stash,"
+ 121 "            Command::new(\"stash\")"
+ 122 "                // pseudo-commits basically"
+ 123 "                // clear stash after a commit"
+ 124 "                // stash create"
+ 125 "                // stash view"
+ 126 "                // stash restore"
+ 127 "                // stash delete"
+ 128 "                .about(\"\"),"
+ 129 "        ),"
+ 130 "        ("
+ 131 "            command_module::restore,"
+ 132 "            Command::new(\"restore\"), // unimplemented"
+ 133 "        ),"
+ 134 "        ("
+ 135 "            command_module::rollback,"
+ 136 "            Command::new(\"rollback\").about(\"Discard all current changes. Rolls back to most recent commit (or pending commit).\"),"
+ 137 "        ),"
+ 138 "        ("
+ 139 "            command_module::cherry,"
+ 140 "            Command::new(\"cherry\").about(\"Go to specific commit.\"),"
+ 141 "        ),"
+ 142 "        ("
+ 143 "            command_module::tree,"
+ 144 "            Command::new(\"tree\").about(\"Generate content tree of current directory.\"),"
+ 145 "        ),"
+ 146 "        ("
+ 147 "            command_module::staging,"
+ 148 "            Command::new(\"staging\").about(\"View all staging changes.\"),"
+ 149 "        ),"
+ 150 "        ("
+ 151 "            command_module::pending,"
+ 152 "            Command::new(\"pending\").about(\"View all pending commits.\")"
+ 153 "                .arg(arg!([COMMIT]... \"Commit number.\"))"
+ 154 "        ),"
+ 155 "        ("
+ 156 "            command_module::qhar,"
+ 157 "            Command::new(\"qhar\").about(\"??\")"
+ 158 "        ),"
+ 159 "        ("
+ 160 "            command_module::test,"
+ 161 "            Command::new(\"test\").about(\"this is here for debug purposes\")"
+ 162 "        )"
+ 163 "    ]) {"
+ 164 "        commands.insert(c.get_name().to_string(), f);"
+ 165 "        command_handler = command_handler.subcommand(c);"
+ 166 "    }"
+ 167 ""
+ 168 "    CommandHandler {"
+ 169 "        handler: command_handler,"
+ 170 "        commands: commands,"
+ 171 "    }"
+ 172 "}"
+ 173 ""
+ 174 "pub fn handle(command_handler: CommandHandler, args: ArgMatches, state: Result<State, RelicError>) {"
+ 175 "    let (command_name, sub_matches) = args.subcommand().unwrap();"
+ 176 ""
+ 177 "    // TODO : shorten and undry this"
+ 178 "    if let Ok(mut s) = state {"
+ 179 "        match command_name {"
+ 180 "            \"clone\" | \"init\" => {"
+ 181 "                // let this run only for"
+ 182 "                // clone, init"
+ 183 "                println!(\"Unable to '{command_name}' an already existing Relic repository.\");"
+ 184 "                return;"
+ 185 "            }"
+ 186 "            _ => match command_handler.commands.get(command_name) {"
+ 187 "                Some(command) => {"
+ 188 "                    command(&mut s, sub_matches);"
+ 189 "                }"
+ 190 "                None => {"
+ 191 "                    unimplemented!(\"Relic Error, command not defined.\");"
+ 192 "                }"
+ 193 "            },"
+ 194 "        }"
+ 195 "    } else {"
+ 196 "        match command_name {"
+ 197 "            \"clone\" | \"init\" => {"
+ 198 "                // let this run only for"
+ 199 "                // clone, init"
+ 200 "                match command_handler.commands.get(command_name) {"
+ 201 "                    Some(command) => {"
+ 202 "                        command(&mut State::empty(), sub_matches);"
+ 203 "                    }"
+ 204 "                    None => {"
+ 205 "                        unimplemented!(\"Relic Error, command not defined.\");"
+ 206 "                    }"
+ 207 "                }"
+ 208 "            }"
+ 209 "            _ => {"
+ 210 "                println!(\"No valid Relic repository found in current directory. Consider executing 'relic init' or 'relic clone'.\");"
+ 211 "                return;"
+ 212 "            }"
+ 213 "        }"
+ 214 "    }"
+ 215 "}"
| .%2Fsrc error.rs
+ 0 "use serde::{Deserialize, Serialize};"
+ 1 ""
+ 2 "#[derive(Debug, Serialize, Deserialize)]"
+ 3 "pub enum RelicError {"
+ 4 "    FileCantOpen,"
+ 5 "    IgnoredFile,"
+ 6 "    ConfigurationIncorrect,"
+ 7 "    RelicInfo(Box<RelicError>),"
+ 8 "}"
| .%2Fsrc lib.rs
+ 0 "pub mod cli;"
+ 1 "pub mod commands;"
+ 2 "pub mod core;"
+ 3 "pub mod error;"
+ 4 "pub mod utils;"
| .%2Fsrc main.rs
+ 0 "use std::path::PathBuf;"
+ 1 ""
+ 2 "mod core;"
+ 3 ""
+ 4 "mod error;"
+ 5 "mod utils;"
+ 6 ""
+ 7 "mod cli;"
+ 8 "mod commands;"
+ 9 ""
+ 10 "use crate::core::state::State;"
+ 11 ""
+ 12 "fn main() {"
+ 13 "    let command_handler = cli::build();"
+ 14 "    let state = State::create(PathBuf::from(\".\"));"
+ 15 "    let args = command_handler.handler.clone().get_matches();"
+ 16 ""
+ 17 "    cli::handle(command_handler, args, state);"
+ 18 "}"
| .%2Fsrc utils.rs
+ 0 "use std::time::{Duration, SystemTime, UNIX_EPOCH};"
+ 1 ""
+ 2 "use chrono::{DateTime, Utc};"
+ 3 ""
+ 4 "use crate::core::{Content, Tree};"
+ 5 ""
+ 6 "pub fn generate_tree(tree: &Tree) -> String {"
+ 7 "    return generate_subtree(&Content::Tree(tree.clone()));"
+ 8 "}"
+ 9 ""
+ 10 "fn generate_subtree(c: &Content) -> String {"
+ 11 "    let mut result = vec![];"
+ 12 ""
+ 13 "    match c {"
+ 14 "        Content::Tree(t) => {"
+ 15 "            let mut r = vec![t.name.clone()];"
+ 16 "            if t.content.len() >= 1 {"
+ 17 "                let length = t.content.len() - 1;"
+ 18 "                for (index, i) in t.content.iter().enumerate() {"
+ 19 "                    for (inner_index, line) in generate_subtree(i).split(\"\\n\").enumerate() {"
+ 20 "                        r.push(format!("
+ 21 "                            \" {} {line}\","
+ 22 "                            if index == length {"
+ 23 "                                if inner_index == 0 {"
+ 24 "                                    \"└\""
+ 25 "                                } else {"
+ 26 "                                    \"\""
+ 27 "                                }"
+ 28 "                            } else {"
+ 29 "                                if inner_index == 0 {"
+ 30 "                                    \"├\""
+ 31 "                                } else {"
+ 32 "                                    \"│\""
+ 33 "                                }"
+ 34 "                            }"
+ 35 "                        ));"
+ 36 "                    }"
+ 37 "                }"
+ 38 "            }"
+ 39 "            result.push(r.join(\"\\n\"));"
+ 40 "        }"
+ 41 "        Content::Blob(b) => {"
+ 42 "            result.push(b.name.clone());"
+ 43 "            // result.push(format!(\"{} ({})\", b.name, sha256::digest(&b.content)));"
+ 44 "        }"
+ 45 "    }"
+ 46 ""
+ 47 "    result.join(\"\\n\")"
+ 48 "}"
+ 49 ""
+ 50 "pub fn get_time() -> u64 {"
+ 51 "    SystemTime::now()"
+ 52 "        .duration_since(UNIX_EPOCH)"
+ 53 "        .expect(\"time went backwards (???)\")"
+ 54 "        .as_millis() as u64"
+ 55 "}"
+ 56 ""
+ 57 "pub fn into_human_readable(t: u64) -> String {"
+ 58 "    // accepts unix time, but only in milliseconds format"
+ 59 "    DateTime::<Utc>::from(UNIX_EPOCH + Duration::from_millis(t as u64))"
+ 60 "        .format(\"%Y-%m-%d %H:%M:%S\")"
+ 61 "        .to_string()"
+ 62 "}"
| .%2Fsrc%2Fcommands add.rs
+ 0 "use std::{collections::HashSet, fs, path::PathBuf};"
+ 1 ""
+ 2 "use clap::ArgMatches;"
+ 3 ""
+ 4 "use crate::core::{paths::RELIC_PATH_TRACKED, state::State};"
+ 5 ""
+ 6 "pub fn add(_: &mut State, args: &ArgMatches) {"
+ 7 "    let f = args"
+ 8 "        .get_many::<PathBuf>(\"FILE\")"
+ 9 "        .unwrap()"
+ 10 "        .map(|x| x.clone())"
+ 11 "        .collect::<Vec<PathBuf>>();"
+ 12 ""
+ 13 "    let mut result: HashSet<String> = HashSet::from_iter("
+ 14 "        fs::read_to_string(format!(\"./{RELIC_PATH_TRACKED}\"))"
+ 15 "            .unwrap()"
+ 16 "            .split(\"\\n\")"
+ 17 "            .filter(|x| !x.is_empty())"
+ 18 "            .map(|x| x.to_string())"
+ 19 "            .collect::<Vec<String>>(),"
+ 20 "    );"
+ 21 "    for p in f {"
+ 22 "        // TODO : path.join for this? or concatenating / works?"
+ 23 "        result.insert(format!("
+ 24 "            \"{}{}\","
+ 25 "            p.to_string_lossy().to_string(),"
+ 26 "            if !p.to_string_lossy().to_string().ends_with(\"/\") && p.is_dir() {"
+ 27 "                \"/\""
+ 28 "            } else {"
+ 29 "                \"\""
+ 30 "            }"
+ 31 "        ));"
+ 32 "    }"
+ 33 "    let _ = fs::write("
+ 34 "        format!(\"./{RELIC_PATH_TRACKED}\"),"
+ 35 "        result.drain().collect::<Vec<String>>().join(\"\\n\"),"
+ 36 "    );"
+ 37 "}"
| .%2Fsrc%2Fcommands branch.rs
+ 0 "use clap::ArgMatches;"
+ 1 ""
+ 2 "use crate::core::state::State;"
+ 3 ""
+ 4 "pub fn branch(_: &mut State, _: &ArgMatches) {}"
| .%2Fsrc%2Fcommands cherry.rs
+ 0 "use clap::ArgMatches;"
+ 1 ""
+ 2 "use crate::core::state::State;"
+ 3 ""
+ 4 "pub fn cherry(_: &mut State, _: &ArgMatches) {}"
| .%2Fsrc%2Fcommands clone.rs
+ 0 "use clap::ArgMatches;"
+ 1 ""
+ 2 "use crate::core::State;"
+ 3 ""
+ 4 "pub fn clone(_: &mut State, args: &ArgMatches) {"
+ 5 "    if let Some(remote) = args.get_one::<String>(\"URL\") {"
+ 6 "        println!(\"remote : {remote}\");"
+ 7 ""
+ 8 "        // validate if remote is a relic repository"
+ 9 "        // probably need some versioning system"
+ 10 ""
+ 11 "        // let _ = fs::create_dir(paths::RELIC_PATH_PARENT);"
+ 12 "        // let _ = fs::create_dir(paths::RELIC_PATH_HISTORY);"
+ 13 "        // let _ = fs::create_dir(paths::RELIC_PATH_PENDING);"
+ 14 "        // let _ = fs::write(paths::RELIC_PATH_INFO, DEFAULT_INFO);"
+ 15 "        // let _ = fs::write(paths::RELIC_PATH_ROOT, \"\");"
+ 16 "        // let _ = fs::write(paths::RELIC_PATH_TRACKED, \"\");"
+ 17 "        // let _ = fs::write(paths::RELIC_PATH_UPSTREAM, DEFAULT_UPSTREAM);"
+ 18 ""
+ 19 "        // let _ = fs::write(paths::RELIC_PATH_IGNORE, content_set::DEFAULT_IGNORE);"
+ 20 "    } else {"
+ 21 "        println!(\"No remote URL provided.\");"
+ 22 "    }"
+ 23 "}"
| .%2Fsrc%2Fcommands commit.rs
+ 0 "use clap::ArgMatches;"
+ 1 ""
+ 2 "use crate::{"
+ 3 "    core::{commit::Commit, state::State},"
+ 4 "    utils,"
+ 5 "};"
+ 6 ""
+ 7 "pub fn commit(state: &mut State, args: &ArgMatches) {"
+ 8 "    // push into pending stage"
+ 9 "    // update upstream"
+ 10 ""
+ 11 "    // everything after the first line will be generated by Change::serialise_change"
+ 12 "    r#\"= {commit id} {unix timestamp of commit} {message} {description} {author}"
+ 13 "+ D \"lorem/ipsum/dolor\""
+ 14 "+ F \"lorem/ipsum/dolor/earth.txt\" \"earth.txt\""
+ 15 "- D \"lorem/sit\""
+ 16 "="
+ 17 "| \"lorem/ipsum/dolor/earth.txt\""
+ 18 "+ 3 asdfsdf"
+ 19 "+ 5 sfsdf"
+ 20 "- 7"
+ 21 "| \"lorem/ipsum/saturn/txt\""
+ 22 "+ 4 lsdfljs\"#;"
+ 23 "    let message = args.get_one::<String>(\"message\").unwrap().clone();"
+ 24 "    let description = args"
+ 25 "        .get_one::<String>(\"description\")"
+ 26 "        .map_or(\"\".to_string(), String::clone);"
+ 27 ""
+ 28 "    let commit = Commit {"
+ 29 "        id: None,"
+ 30 "        message,"
+ 31 "        description,"
+ 32 "        change: state.get_changes(),"
+ 33 "        timestamp: utils::get_time(),"
+ 34 "        author: \"no_one\".to_string(),"
+ 35 "    };"
+ 36 ""
+ 37 "    state.pending_add(commit);"
+ 38 "    // update upstream"
+ 39 "    (*state).update_upstream(&mut state.track_set.clone());"
+ 40 "}"
| .%2Fsrc%2Fcommands detach.rs
+ 0 "use std::fs;"
+ 1 ""
+ 2 "use clap::ArgMatches;"
+ 3 ""
+ 4 "use crate::core::{paths, State};"
+ 5 ""
+ 6 "pub fn detach(_: &mut State, _: &ArgMatches) {"
+ 7 "    let _ = fs::remove_dir_all(paths::RELIC_PATH_PARENT);"
+ 8 "    let _ = fs::remove_file(paths::RELIC_PATH_IGNORE);"
+ 9 ""
+ 10 "    println!(\"Relic repository successfully removed.\");"
+ 11 "}"
| .%2Fsrc%2Fcommands fetch.rs
+ 0 "use clap::ArgMatches;"
+ 1 ""
+ 2 "use crate::core::state::State;"
+ 3 ""
+ 4 "pub fn fetch(_: &mut State, _: &ArgMatches) {}"
| .%2Fsrc%2Fcommands init.rs
+ 0 "use std::fs;"
+ 1 ""
+ 2 "use clap::ArgMatches;"
+ 3 ""
+ 4 "use crate::core::{content_set, objects::data::Upstream, paths, state, State};"
+ 5 ""
+ 6 "pub fn init(_: &mut State, _: &ArgMatches) {"
+ 7 "    // create"
+ 8 "    // .relic"
+ 9 "    //      history/ (empty)"
+ 10 "    //      pending/ (empty)"
+ 11 "    //      root (empty)"
+ 12 "    //      tracked (empty)"
+ 13 "    //      upstream (empty)"
+ 14 "    // .relic_ignore (use default (const in content_set))"
+ 15 ""
+ 16 "    // if origin is set"
+ 17 "    // update root"
+ 18 "    // update upstream"
+ 19 ""
+ 20 "    let _ = fs::create_dir(paths::RELIC_PATH_PARENT);"
+ 21 "    let _ = fs::create_dir(paths::RELIC_PATH_HISTORY);"
+ 22 "    let _ = fs::create_dir(paths::RELIC_PATH_PENDING);"
+ 23 "    let _ = fs::write(paths::RELIC_PATH_INFO, state::DEFAULT_INFO);"
+ 24 "    let _ = fs::write(paths::RELIC_PATH_ROOT, \"\");"
+ 25 "    let _ = fs::write(paths::RELIC_PATH_TRACKED, \"\");"
+ 26 "    let _ = fs::write(paths::RELIC_PATH_UPSTREAM, Upstream::empty().serialise());"
+ 27 ""
+ 28 "    let _ = fs::write(paths::RELIC_PATH_IGNORE, content_set::DEFAULT_IGNORE);"
+ 29 ""
+ 30 "    println!(\"Empty Relic repository created.\");"
+ 31 "}"
| .%2Fsrc%2Fcommands mod.rs
+ 0 "pub mod add;"
+ 1 "pub mod branch;"
+ 2 "pub mod cherry;"
+ 3 "pub mod clone;"
+ 4 "pub mod commit;"
+ 5 "pub mod detach;"
+ 6 "pub mod fetch;"
+ 7 "pub mod init;"
+ 8 "pub mod pending;"
+ 9 "pub mod pull;"
+ 10 "pub mod push;"
+ 11 "pub mod qhar;"
+ 12 "pub mod remove;"
+ 13 "pub mod rollback;"
+ 14 "pub mod staging;"
+ 15 "pub mod stash;"
+ 16 "pub mod test;"
+ 17 "pub mod tree;"
+ 18 ""
+ 19 "pub use add::add;"
+ 20 "pub use branch::branch;"
+ 21 "pub use cherry::cherry;"
+ 22 "pub use clone::clone;"
+ 23 "pub use commit::commit;"
+ 24 "pub use detach::detach;"
+ 25 "pub use fetch::fetch;"
+ 26 "pub use init::init;"
+ 27 "pub use pending::pending;"
+ 28 "pub use pull::pull;"
+ 29 "pub use push::push;"
+ 30 "pub use qhar::qhar;"
+ 31 "pub use remove::remove;"
+ 32 "pub use rollback::rollback;"
+ 33 "pub use staging::staging;"
+ 34 "pub use stash::{restore, stash};"
+ 35 "pub use test::test;"
+ 36 "pub use tree::tree;"
| .%2Fsrc%2Fcommands pending.rs
+ 0 "use clap::ArgMatches;"
+ 1 ""
+ 2 "use crate::core::state::State;"
+ 3 ""
+ 4 "pub fn pending(state: &mut State, args: &ArgMatches) {"
+ 5 "    let pending = state.pending_get();"
+ 6 ""
+ 7 "    if let Some(commit_number) = args"
+ 8 "        .get_one::<String>(\"COMMIT\")"
+ 9 "        .map_or(None, |x| x.parse::<i32>().map_or(None, |x| Some(x)))"
+ 10 "    {"
+ 11 "        // display selected"
+ 12 "        if (commit_number < 0) || (commit_number >= pending.len() as i32) {"
+ 13 "            println!("
+ 14 "                \"Invalid selection. Please select commit numbers in the range of (0-{})\","
+ 15 "                pending.len() - 1"
+ 16 "            );"
+ 17 "            return;"
+ 18 "        }"
+ 19 ""
+ 20 "        println!(\"{}\", pending[commit_number as usize].serialise());"
+ 21 "    } else {"
+ 22 "        // display all"
+ 23 "        for (index, c) in pending.iter().enumerate() {"
+ 24 "            println!(\"{index}. {}\", c.header());"
+ 25 "        }"
+ 26 "    }"
+ 27 "}"
| .%2Fsrc%2Fcommands pull.rs
+ 0 "use clap::ArgMatches;"
+ 1 ""
+ 2 "use crate::core::state::State;"
+ 3 ""
+ 4 "pub fn pull(_: &mut State, _: &ArgMatches) {}"
| .%2Fsrc%2Fcommands push.rs
+ 0 "use clap::ArgMatches;"
+ 1 ""
+ 2 "use crate::core::state::State;"
+ 3 ""
+ 4 "pub fn push(_: &mut State, _: &ArgMatches) {}"
| .%2Fsrc%2Fcommands qhar.rs
+ 0 "use clap::ArgMatches;"
+ 1 ""
+ 2 "use crate::core::State;"
+ 3 ""
+ 4 "pub fn qhar(_: &mut State, _: &ArgMatches) {"
+ 5 "    println!(\".................................................................................................\\n.................................................................................................\\n.................................................................................................\\n.....-----:......:-:.......:--........:-:.......:------::......:----:....:-:....:---::...:::.....\\n..:-+#%%%%+=:...:+#=:......=#+:......-*#+:......=%%%%%%#*=:..:=*%%%%*-:.-+#=..-+#%%%#+-.:=#*-....\\n.:-%%#+=+#%%+:..-*%+:.....:=%*:.....:+%@#-.....:+%%++++#%%*-.=#%#+=#@#-.-#%=::+%%+=+%%+::+%#-....\\n.-#%=:...:=%%=..-*%+:.....:=%*:.....=#%#%*:....:+%*:...:=%%=-+#+:..-%%=.-*%=:-*#=..:=%#-:+%#-....\\n:+@*:.....:+%#-.-#%+-.....:+%*:....:+%*=#%-....:+%*-....-#%+:::....-%%=:-#%=:.::...:=%#-:+%#-....\\n-#@=:.....:=%%-.-#%+-------+%*:....-##=:+%+:...:+%*:...:=%%=:.....-+%*-.-*%=:.....:=#%+:.=%#-....\\n-#@=......:=%%=.-#@%%######%@*:...:+%+:.-##-:..:+%#====+#%*-.....-*%*-..-*%=.....:=#%+-.:=%#-....\\n-#@=:......=%%=.-#@#+======*@*:..:=##=::-+@*-..:+@%##%%@#=:.....-#%*-:..-*%=....:+%#=:..:=%#-....\\n-*@=:.....:=%%-.:#@+-.....:+%*:..:+@%*++*#@%=..:+%#=--=@#=:....:=%#-....-*#=....-#%+:....=%*-....\\n:+@*-.....-*%#:.:#@+:.....:=%*:..=#%######%@*-.:+%*:..:*%*-:...:-+=:....:==-....:++-.....-==:....\\n.-#%+-:.:-+%%=..:#%+-.....:=%*:.-*%*::::..=#%=::+%*-...-#%+-.....::......:::.....::.......::.....\\n.:-%%%*++#%%=:..-*%+:......+%+-:=%#=:.....-+@*-:+%*-....=%%=:..:=#+-...:-+*=:...:*#=:....=*+-....\\n...:=*#%%%@#=...:+*=:......=#=::+#+-.:.::.:-**-:=*=:.::::=#*-:..=*+-....:+*=....:**=.....=*+-....\\n....:::::-+%%+:..::........::-::---:::::::::-=-:::::--===+++==-::::......::......::.......::.....\\n..........:-=-.............:----==--:::::----===-:-=++*####++++=-:::----:::::::..................\\n..:........................:---===-----------==+=--=***+==+***++=--=+++++=--===-.................\\n.....::::::................:-====-------------===-:-------=+*****+++===++*+***+=:................\\n...:-==++++=--..::---:.....:-=======--======-:-=-......:..::-==++==-:::---====-::................\\n...:=+**#*+++=--==+++=-....-=--============-::-=-.......::::::-::::::::::.::::......:::..........\\n...::--==+++++++++***+-::.:------====+++===-----:......-===++====---====-:........:-===---::::...\\n......::=+**######**+-::...:--========*#+=---::...:::::=+++*####****####*=-:::.:.:=+#***+++===-:.\\n......::-==+++++==-:::...:...:--===---=*+-::.:......::-=********+++++****+++====---=++*+****+++==\\n..........::::::::::::....:..:.::-==---+*=::...::.:::::-=+*####+=---+###****#**++++=-----=+***+++\\n............::--======-------===-------+#+-:::::::::.::::-----=-:::--++*#######*+++++=-:::-=+*###\\n..........:.-=+++******++++=+***+=-----=*#-------:::::::-----------=+++********++****+=:::::--=++\\n....:.:...::-+***##########*+###+=-----=+#+=====----------=======++*################*+--::.:.::::\\n..::::::..::-+*****#+=======-==---=======++====----=--=======+*###****##*+++++++++===-------:::..\\n:-=====--:::-=+**###+---------------==++++====---------=======+*#####*++=-::-::::::::-=++++=+==--\\n-++*#*++==--==++====---:::--::::::--==========--------======---=++##++=--:::::::::::-=+*****##**+\\n:-=+++++*+=+++**=--::::::::--------======--------------=====--=+****+++==--:::::::--=+**+**######\\n.::-=++++++**#**=--::::::----========------------------------=+####*++++++++=-::-=++***###******+\\n.::-=+**+*##*+=--:::::::--:--========-----=======-----:--------==+**+*+++***+=--+#*******########\\n.:::-=======--:::::::::::-----=========++#*******+=--:::::::::::----==++****+++=+#*****++===+++++\\n.:.:::::::::::::::::::::::----======++############*+--::::::::::::::---=********+******+=--::::::\\n....:.::::---=====---::::------====+*#############**+=--::::::::::::::--=*#************=--::::::.\\n.:::---==++++++**++++=--::--------=+#######*+++##******=======--::::::::-=+*#########*+=======--:\\n:--=+++**************++---::::-----+*######*===+*#***#*+++****++-:::::::::--===++++==*******+++++\\n=++*****######********+--:::::::---=*#####*+=--=+**###+*#******+-::::::::.:::::::::-=+#########**\\n+******###************+--:::::::::--=+***+=-------=++==*#####*+=-:::..:..::::..:::::--=+*#######*\\n******##*==++********+=--:::::::::::---==--::::::------===++++=-:::::...:..:.::..:::-==*+********\\n#######*==+*********+--:::::::.::::::::..:::::--=+**+=---::::::::...::........:::--=+###*********\\n##**+==--=+*****###+=--::::...::.:..:.::..:::-++*****#*=--:::::....:...:..:.:::-=++**############\\n=--------=+########*+=--::::::..:.:.:..:::::-=**#**####**+=-::::::::--:..::.::-=*#*******#*++++++\\n::::::::--+#####******+=-:::.:.............::-==+#####*****+-::::-==++=-::.:::-=*####***+=-------\\n.:.:.::::--+**********#*+-:::............::.:::--==++**#****=--=++*****+-:::.::--=++*#*==--::::::\\n.......:::-=+****#*####*+-::::..............::::::--=+******=--+*#*#****=-:::.::::--==---::::....\\n......::::--=+*######*+==-::.........::.:.....:::::-=+******+===****#***=-:::::.::::::::.........\\n......:.::::--=***+==--:::...............::.....:::-=+**##********##***+=-:::....................\\n....:::.:::::--=---:::::.......................:.::-=+#**####****##***+=--::.:...................\\n................::.::..........................:.::--=*##########***++=-:::......................\");"
+ 6 "}"
| .%2Fsrc%2Fcommands remove.rs
+ 0 "use std::{collections::HashSet, fs, path::PathBuf};"
+ 1 ""
+ 2 "use clap::ArgMatches;"
+ 3 ""
+ 4 "use crate::core::{"
+ 5 "    content_set::{ContentSet, TrackingSet},"
+ 6 "    paths::RELIC_PATH_TRACKED,"
+ 7 "    state::State,"
+ 8 "};"
+ 9 ""
+ 10 "pub fn remove(s: &mut State, args: &ArgMatches) {"
+ 11 "    let f = args"
+ 12 "        .get_many::<PathBuf>(\"FILE\")"
+ 13 "        .unwrap()"
+ 14 "        .map(|x| x.clone())"
+ 15 "        .collect::<Vec<PathBuf>>();"
+ 16 ""
+ 17 "    let result: HashSet<String> = HashSet::from_iter("
+ 18 "        fs::read_to_string(format!(\"./{RELIC_PATH_TRACKED}\"))"
+ 19 "            .unwrap()"
+ 20 "            .split(\"\\n\")"
+ 21 "            .filter(|x| !x.is_empty())"
+ 22 "            .map(|x| PathBuf::from(\".\").join(x).to_string_lossy().to_string())"
+ 23 "            .collect::<Vec<String>>(),"
+ 24 "    );"
+ 25 ""
+ 26 "    // initialise removed_content"
+ 27 "    let mut removed_content = ContentSet {"
+ 28 "        files: HashSet::from_iter("
+ 29 "            f.iter()"
+ 30 "                .filter(|x| !x.is_dir())"
+ 31 "                .map(|x| PathBuf::from(\".\").join(x).to_string_lossy().to_string()),"
+ 32 "        ),"
+ 33 "        directories: HashSet::from_iter("
+ 34 "            f.iter()"
+ 35 "                .filter(|x| x.is_dir())"
+ 36 "                .map(|x| PathBuf::from(\".\").join(x).to_string_lossy().to_string()),"
+ 37 "        ),"
+ 38 "    }"
+ 39 "    .initialise(&mut s.current);"
+ 40 ""
+ 41 "    let mut to_subtract: HashSet<String> = HashSet::from_iter("
+ 42 "        removed_content"
+ 43 "            .directories"
+ 44 "            .drain()"
+ 45 "            .collect::<Vec<String>>()"
+ 46 "            .into_iter()"
+ 47 "            .map(|x| format!(\"{x}/\"))"
+ 48 "            .collect::<Vec<String>>(),"
+ 49 "    );"
+ 50 "    to_subtract = to_subtract"
+ 51 "        .union(&HashSet::from_iter(removed_content.files.drain()))"
+ 52 "        .map(|x| x.to_string())"
+ 53 "        .collect::<HashSet<String>>();"
+ 54 ""
+ 55 "    // set operations"
+ 56 "    // right join"
+ 57 "    // result - removed_content"
+ 58 ""
+ 59 "    let _ = fs::write("
+ 60 "        format!(\"./{RELIC_PATH_TRACKED}\"),"
+ 61 "        result"
+ 62 "            .difference(&to_subtract)"
+ 63 "            .map(|x| x[2..].to_string())"
+ 64 "            .collect::<Vec<String>>()"
+ 65 "            .join(\"\\n\"),"
+ 66 "    );"
+ 67 "}"
| .%2Fsrc%2Fcommands rollback.rs
+ 0 "use clap::ArgMatches;"
+ 1 ""
+ 2 "use crate::core::state::State;"
+ 3 ""
+ 4 "pub fn rollback(_: &mut State, _: &ArgMatches) {}"
| .%2Fsrc%2Fcommands staging.rs
+ 0 "use clap::ArgMatches;"
+ 1 ""
+ 2 "use crate::core::{content_set::TrackingSet, State};"
+ 3 ""
+ 4 "pub fn staging(s: &mut State, _: &ArgMatches) {"
+ 5 "    println!("
+ 6 "        \"{}\","
+ 7 "        s.get_changes()"
+ 8 "            .filter_changes(&s.track_set.initialise(&mut s.current))"
+ 9 "            .serialise_changes()"
+ 10 "    );"
+ 11 "}"
| .%2Fsrc%2Fcommands stash.rs
+ 0 "use clap::ArgMatches;"
+ 1 ""
+ 2 "use crate::core::State;"
+ 3 ""
+ 4 "pub fn stash(_: &mut State, _: &ArgMatches) {}"
+ 5 ""
+ 6 "pub fn restore(_: &mut State, _: &ArgMatches) {}"
| .%2Fsrc%2Fcommands test.rs
+ 0 "use clap::ArgMatches;"
+ 1 ""
+ 2 "use crate::core::State;"
+ 3 ""
+ 4 "pub fn test(s: &mut State, _: &ArgMatches) {"
+ 5 "    println!(\"{:?}\", s.info);"
+ 6 "}"
| .%2Fsrc%2Fcommands tree.rs
+ 0 "use clap::ArgMatches;"
+ 1 ""
+ 2 "use crate::{core::State, utils};"
+ 3 ""
+ 4 "pub fn tree(s: &mut State, _: &ArgMatches) {"
+ 5 "    println!(\"{}\", utils::generate_tree(&s.current));"
+ 6 "}"
| .%2Fsrc%2Fcore commit.rs
+ 0 "use crate::{core::modifications::Change, utils};"
+ 1 ""
+ 2 "const PENDING_TAG: &str = \"LOCAL\";"
+ 3 ""
+ 4 "#[derive(Debug, Clone)]"
+ 5 "pub struct Commit {"
+ 6 "    pub id: Option<u32>,"
+ 7 "    pub message: String,"
+ 8 "    pub description: String,"
+ 9 "    pub change: Change,"
+ 10 "    pub timestamp: u64,"
+ 11 ""
+ 12 "    pub author: String,"
+ 13 "}"
+ 14 "impl Commit {"
+ 15 "    pub fn hash(&self) -> String {"
+ 16 "        sha256::digest(self.serialise())"
+ 17 "    }"
+ 18 ""
+ 19 "    pub fn header(&self) -> String {"
+ 20 "        // \"integrated backwards compatibility\" (2025-5-26 16:30) (affected : change.rs, content.rs, ...)"
+ 21 ""
+ 22 "        let mut file_names = vec![];"
+ 23 "        for (_, parent) in self.change.as_map().1 {"
+ 24 "            for (f, _) in parent {"
+ 25 "                file_names.push(f);"
+ 26 "            }"
+ 27 "        }"
+ 28 ""
+ 29 "        format!("
+ 30 "            \"({}) \\\"{}\\\" (affected : {}{})\","
+ 31 "            utils::into_human_readable(self.timestamp),"
+ 32 "            self.message,"
+ 33 "            file_names"
+ 34 "                .iter()"
+ 35 "                .take(5)"
+ 36 "                .map(|x| x.to_string())"
+ 37 "                .collect::<Vec<String>>()"
+ 38 "                .join(\", \"),"
+ 39 "            if file_names.len() > 5 { \", ...\" } else { \"\" }"
+ 40 "        )"
+ 41 "    }"
+ 42 ""
+ 43 "    pub fn serialise(&self) -> String {"
+ 44 "        format!("
+ 45 "            \"= {} {} {:?} {:?} {}\\n{}\","
+ 46 "            self.id"
+ 47 "                .map_or(PENDING_TAG.to_string(), |i| format!(\"{:06x}\", i).clone()),"
+ 48 "            self.timestamp,"
+ 49 "            urlencoding::encode(&self.message).to_string(),"
+ 50 "            urlencoding::encode(&self.description).to_string(),"
+ 51 "            self.author,"
+ 52 "            self.change.serialise_changes()"
+ 53 "        )"
+ 54 "    }"
+ 55 ""
+ 56 "    pub fn deserialise(s: String) -> Option<Commit> {"
+ 57 "        // = LOCAL 1747682692319414000 \"initial%20commit\" \"\" no_one"
+ 58 ""
+ 59 "        let lines = s.split(\"\\n\").collect::<Vec<&str>>();"
+ 60 "        if lines.len() < 2 {"
+ 61 "            // return None;"
+ 62 "        }"
+ 63 ""
+ 64 "        let metadata = lines[0].split(\" \").collect::<Vec<&str>>();"
+ 65 "        if metadata.len() != 6 {"
+ 66 "            // return None;"
+ 67 "        }"
+ 68 ""
+ 69 "        let [_, status, time, message, description, author] = *metadata.as_slice() else {"
+ 70 "            return None;"
+ 71 "        };"
+ 72 ""
+ 73 "        Some(Commit {"
+ 74 "            id: status.parse::<u32>().map_or(None, |t| Some(t)),"
+ 75 "            message: urlencoding::decode(&message[1..message.len() - 1].to_string())"
+ 76 "                .unwrap()"
+ 77 "                .to_string(),"
+ 78 "            description: urlencoding::decode(&description[1..description.len() - 1].to_string())"
+ 79 "                .unwrap()"
+ 80 "                .to_string(),"
+ 81 "            change: Change::deserialise_changes(lines[1..].join(\"\\n\")).unwrap_or(Change::empty()),"
+ 82 "            timestamp: time.parse::<u64>().unwrap_or(0),"
+ 83 "            author: author.to_string(),"
+ 84 "        })"
+ 85 "    }"
+ 86 "}"
| .%2Fsrc%2Fcore mod.rs
+ 0 "pub mod paths;"
+ 1 ""
+ 2 "pub mod objects;"
+ 3 ""
+ 4 "pub mod relic;"
+ 5 "pub mod relic_info;"
+ 6 "pub mod state;"
+ 7 ""
+ 8 "pub mod commit;"
+ 9 ""
+ 10 "pub use objects::{content_set, modifications, Content, ContentMutRef, Tree, Blob};"
+ 11 "pub use relic_info::RelicInfo;"
+ 12 "pub use state::State;"
| .%2Fsrc%2Fcore paths.rs
+ 0 "pub const RELIC_PATH_PARENT: &str = \".relic\";"
+ 1 "pub const RELIC_PATH_HISTORY: &str = \".relic/history\";"
+ 2 "pub const RELIC_PATH_PENDING: &str = \".relic/pending\";"
+ 3 ""
+ 4 "pub const RELIC_PATH_ROOT: &str = \".relic/root\";"
+ 5 "pub const RELIC_PATH_INFO: &str = \".relic/info.json\";"
+ 6 "pub const RELIC_PATH_TRACKED: &str = \".relic/tracked\";"
+ 7 "pub const RELIC_PATH_UPSTREAM: &str = \".relic/upstream\";"
+ 8 ""
+ 9 "pub const RELIC_PATH_IGNORE: &str = \".relic_ignore\";"
| .%2Fsrc%2Fcore relic.rs
+ 0 "// use crate::state::State;"
+ 1 ""
+ 2 "// #[derive(Debug)]"
+ 3 "// pub struct Relic {"
+ 4 "//     // holds"
+ 5 "//     //      history.changes"
+ 6 "//     //      now.changes"
+ 7 "//     //      root"
+ 8 "//     //      upstream"
+ 9 "//     pub upstream: State,"
+ 10 "// }"
+ 11 "// impl Relic {"
+ 12 "//     pub fn empty() -> Relic {"
+ 13 "//         Relic {"
+ 14 "//             upstream: State::empty(),"
+ 15 "//         }"
+ 16 "//     }"
+ 17 "// }"
| .%2Fsrc%2Fcore relic_info.rs
+ 0 "use std::fs;"
+ 1 ""
+ 2 "use serde::{Deserialize, Serialize};"
+ 3 ""
+ 4 "use crate::{core::paths::RELIC_PATH_INFO, error::RelicError};"
+ 5 ""
+ 6 "#[derive(Serialize, Deserialize, Clone, Debug)]"
+ 7 "pub struct RelicInfo {"
+ 8 "    pub remote: String,"
+ 9 "    pub branch: String,"
+ 10 "}"
+ 11 "impl RelicInfo {"
+ 12 "    pub fn empty() -> RelicInfo {"
+ 13 "        RelicInfo {"
+ 14 "            remote: \"\".to_string(),"
+ 15 "            branch: \"\".to_string(),"
+ 16 "        }"
+ 17 "    }"
+ 18 ""
+ 19 "    pub fn initialise() -> Result<RelicInfo, RelicError> {"
+ 20 "        if let Ok(t) = fs::read_to_string(format!(\"./{RELIC_PATH_INFO}\")) {"
+ 21 "            if let Ok(d) = serde_json::from_str::<RelicInfo>(&t) {"
+ 22 "                return Ok(d);"
+ 23 "            }"
+ 24 "            return Err(RelicError::RelicInfo(Box::new("
+ 25 "                RelicError::ConfigurationIncorrect,"
+ 26 "            )));"
+ 27 "        }"
+ 28 "        Err(RelicError::RelicInfo(Box::new(RelicError::FileCantOpen)))"
+ 29 "    }"
+ 30 "}"
| .%2Fsrc%2Fcore state.rs
+ 0 "use serde::{Deserialize, Serialize};"
+ 1 "use std::{"
+ 2 "    collections::HashSet,"
+ 3 "    fs,"
+ 4 "    path::{Path, PathBuf},"
+ 5 "};"
+ 6 ""
+ 7 "use crate::{"
+ 8 "    core::{"
+ 9 "        commit::Commit,"
+ 10 "        content_set::{ContentSet, IgnoreSet, TrackingSet},"
+ 11 "        modifications::Change,"
+ 12 "        objects::data::upstream::{self, Upstream},"
+ 13 "        paths::{RELIC_PATH_IGNORE, RELIC_PATH_PENDING, RELIC_PATH_TRACKED, RELIC_PATH_UPSTREAM},"
+ 14 "        Blob, Content, RelicInfo, Tree,"
+ 15 "    },"
+ 16 "    error::RelicError,"
+ 17 "};"
+ 18 ""
+ 19 "// TODO: throw these into constructors, not hard coded"
+ 20 "pub const DEFAULT_INFO: &str = r#\"{"
+ 21 "    \"remote\":\"\","
+ 22 "    \"branch\":\"main\""
+ 23 "}\"#;"
+ 24 ""
+ 25 "#[derive(Debug, Serialize, Deserialize)]"
+ 26 "pub struct State {"
+ 27 "    pub info: RelicInfo,"
+ 28 "    // current & upstream uses a Tree with unset path & name values"
+ 29 "    pub current: Tree,"
+ 30 "    pub upstream: Tree,"
+ 31 "    pub path: PathBuf,"
+ 32 "    pub track_set: ContentSet,"
+ 33 "    pub ignore_set: ContentSet,"
+ 34 "}"
+ 35 ""
+ 36 "impl State {"
+ 37 "    pub fn empty() -> State {"
+ 38 "        // needs to store current upstream commit"
+ 39 "        // local commits assigned an id?"
+ 40 "        State {"
+ 41 "            info: RelicInfo::empty(),"
+ 42 "            current: Tree::new(),"
+ 43 "            upstream: Tree::new(),"
+ 44 "            path: PathBuf::from(\".\"),"
+ 45 "            track_set: ContentSet::empty(),"
+ 46 "            ignore_set: ContentSet::empty(),"
+ 47 "        }"
+ 48 "    }"
+ 49 ""
+ 50 "    pub fn create(path: PathBuf) -> Result<State, RelicError> {"
+ 51 "        let info = match RelicInfo::initialise() {"
+ 52 "            Ok(r) => r,"
+ 53 "            Err(e) => return Err(e),"
+ 54 "        };"
+ 55 ""
+ 56 "        let ignore_set ="
+ 57 "            IgnoreSet::create(fs::read_to_string(RELIC_PATH_IGNORE).unwrap_or(\"\".to_string()));"
+ 58 ""
+ 59 "        let current ="
+ 60 "            match State::content_at(&path.to_string_lossy().to_string(), &path, &ignore_set)? {"
+ 61 "                Content::Tree(t) => t,"
+ 62 "                _ => return Err(RelicError::ConfigurationIncorrect),"
+ 63 "            };"
+ 64 ""
+ 65 "        let upstream = match Upstream::deserialise(RELIC_PATH_UPSTREAM) {"
+ 66 "            Ok(u) => u,"
+ 67 "            Err(e) => return Err(e),"
+ 68 "        }"
+ 69 "        .tree();"
+ 70 ""
+ 71 "        let mut track_set: ContentSet = match fs::read_to_string(RELIC_PATH_TRACKED) {"
+ 72 "            Ok(data) => TrackingSet::deserialise(data),"
+ 73 "            Err(_) => return Err(RelicError::ConfigurationIncorrect),"
+ 74 "        };"
+ 75 ""
+ 76 "        track_set.directories = HashSet::from_iter("
+ 77 "            track_set"
+ 78 "                .directories"
+ 79 "                .difference(&ignore_set.directories)"
+ 80 "                .map(|x| {"
+ 81 "                    PathBuf::from(\".\")"
+ 82 "                        .join(PathBuf::from(x))"
+ 83 "                        .to_string_lossy()"
+ 84 "                        .to_string()"
+ 85 "                }),"
+ 86 "        );"
+ 87 "        track_set.files ="
+ 88 "            HashSet::from_iter(track_set.files.difference(&ignore_set.files).map(|x| {"
+ 89 "                PathBuf::from(\".\")"
+ 90 "                    .join(PathBuf::from(x))"
+ 91 "                    .to_string_lossy()"
+ 92 "                    .to_string()"
+ 93 "            }));"
+ 94 ""
+ 95 "        Ok(State {"
+ 96 "            info,"
+ 97 "            current,"
+ 98 "            upstream,"
+ 99 "            path,"
+ 100 "            track_set,"
+ 101 "            ignore_set,"
+ 102 "        })"
+ 103 "    }"
+ 104 ""
+ 105 "    pub fn content_at("
+ 106 "        file_name: &String,"
+ 107 "        root_path: &PathBuf,"
+ 108 "        ignore_set: &ContentSet,"
+ 109 "    ) -> Result<Content, RelicError> {"
+ 110 "        // get all files at path"
+ 111 "        let paths = match fs::read_dir(root_path) {"
+ 112 "            // let paths = match fs::read_dir(format!(\"./{}\", root_path.clone())) {"
+ 113 "            Ok(r) => r,"
+ 114 "            Err(e) => {"
+ 115 "                println!(\"state.rs (content_at) get all dirs : {root_path:?} : {e:?}\");"
+ 116 "                return Err(RelicError::FileCantOpen);"
+ 117 "            }"
+ 118 "        };"
+ 119 ""
+ 120 "        let mut tree_contents = vec![];"
+ 121 ""
+ 122 "        // iterate through them all"
+ 123 "        for path in paths {"
+ 124 "            match path {"
+ 125 "                Ok(p) => {"
+ 126 "                    let file_type = p.file_type().unwrap();"
+ 127 "                    let file_name = p.file_name().into_string().unwrap();"
+ 128 "                    let file_path = p.path();"
+ 129 ""
+ 130 "                    // if file_name.starts_with(\".\") {"
+ 131 "                    //     continue;"
+ 132 "                    // }"
+ 133 ""
+ 134 "                    if file_type.is_dir() {"
+ 135 "                        if ignore_set.directories.contains(&file_name) {"
+ 136 "                            continue;"
+ 137 "                        }"
+ 138 ""
+ 139 "                        match State::content_at(&file_name, &file_path, ignore_set) {"
+ 140 "                            Ok(c) => {"
+ 141 "                                tree_contents.push(c);"
+ 142 "                            }"
+ 143 "                            Err(e) => {"
+ 144 "                                println!(\"state.rs (content_at) subtraverse : {e:?}\");"
+ 145 "                            }"
+ 146 "                        }"
+ 147 "                    } else if file_type.is_file() {"
+ 148 "                        if ignore_set.files.contains(&file_name) {"
+ 149 "                            continue;"
+ 150 "                        }"
+ 151 ""
+ 152 "                        match Blob::create(file_name, file_path) {"
+ 153 "                            Ok(b) => {"
+ 154 "                                tree_contents.push(Content::Blob(b));"
+ 155 "                            }"
+ 156 "                            _ => {}"
+ 157 "                        }"
+ 158 "                    } else if file_type.is_symlink() {"
+ 159 "                        // TODO : decide what to do here"
+ 160 "                        if ignore_set.files.contains(&file_name) {"
+ 161 "                            continue;"
+ 162 "                        }"
+ 163 "                    }"
+ 164 "                }"
+ 165 "                Err(e) => {"
+ 166 "                    println!(\"state.rs (content_at) read_dir : {e:?}\");"
+ 167 "                }"
+ 168 "            }"
+ 169 "        }"
+ 170 ""
+ 171 "        // println!(\"CREATION : {root_path:?}\");"
+ 172 "        Ok(Content::Tree(Tree {"
+ 173 "            path: root_path.clone(),"
+ 174 "            name: file_name.clone(),"
+ 175 "            content: tree_contents,"
+ 176 "        }))"
+ 177 "    }"
+ 178 ""
+ 179 "    pub fn serialise_state(self: &State) -> String {"
+ 180 "        serde_json::to_string(self).unwrap()"
+ 181 "    }"
+ 182 ""
+ 183 "    pub fn deserialise_state(s: String) -> Option<State> {"
+ 184 "        match serde_json::from_str(&s) {"
+ 185 "            Ok(s) => Some(s),"
+ 186 "            Err(_) => None,"
+ 187 "        }"
+ 188 "    }"
+ 189 ""
+ 190 "    // #region changes"
+ 191 "    pub fn get_changes(&self) -> Change {"
+ 192 "        Change::get_change_all(&self.upstream, &self.current, Path::new(&self.path))"
+ 193 "    }"
+ 194 "    // #endregion"
+ 195 ""
+ 196 "    // #region upstream"
+ 197 "    pub fn update_upstream(&mut self, tracked_content: &ContentSet) {"
+ 198 "        // fully fill tracked_content"
+ 199 "        // eg : \"lorem/\" -> [\"lorem/ipsum\", \"lorem/dolor\", \"lorem/sit\"]"
+ 200 "        // traverse directories and fetch all children"
+ 201 ""
+ 202 "        let tracked_content = tracked_content.clone().initialise(&mut self.current);"
+ 203 ""
+ 204 "        // get changes"
+ 205 "        // filter to only changes in the tracked_content content set"
+ 206 "        let changes = self.get_changes().filter_changes(&tracked_content);"
+ 207 ""
+ 208 "        // apply changes to current"
+ 209 "        self.upstream.apply_changes(changes);"
+ 210 "        let _ = fs::write("
+ 211 "            RELIC_PATH_UPSTREAM,"
+ 212 "            Upstream::from_tree(&self.upstream).serialise(),"
+ 213 "        );"
+ 214 "    }"
+ 215 "    // #endregion"
+ 216 ""
+ 217 "    // #region pending"
+ 218 "    pub fn pending_add(&self, commit: Commit) {"
+ 219 "        // TODO : use numbering for blob name"
+ 220 "        // who knows if two commits are created in the same nanosecond"
+ 221 "        let _ = fs::write("
+ 222 "            format!(\"{RELIC_PATH_PENDING}/{}.diff\", commit.timestamp),"
+ 223 "            commit.serialise(),"
+ 224 "        );"
+ 225 "    }"
+ 226 ""
+ 227 "    pub fn pending_get(&self) -> Vec<Commit> {"
+ 228 "        let directories = if let Ok(d) = fs::read_dir(RELIC_PATH_PENDING) {"
+ 229 "            d"
+ 230 "        } else {"
+ 231 "            return vec![];"
+ 232 "        };"
+ 233 ""
+ 234 "        let mut result = vec![];"
+ 235 ""
+ 236 "        for d in directories {"
+ 237 "            let d = if let Ok(d) = d { d } else { continue };"
+ 238 "            let p = if let Ok(p) = fs::read_to_string(d.path()) {"
+ 239 "                p"
+ 240 "            } else {"
+ 241 "                continue;"
+ 242 "            };"
+ 243 ""
+ 244 "            if let Some(c) = Commit::deserialise(p) {"
+ 245 "                result.push(c);"
+ 246 "            }"
+ 247 "        }"
+ 248 ""
+ 249 "        result.sort_by_key(|c| c.timestamp);"
+ 250 ""
+ 251 "        result"
+ 252 "    }"
+ 253 "    // #endregion"
+ 254 "}"
| .%2Fsrc%2Fcore%2Fobjects mod.rs
+ 0 "pub mod data;"
+ 1 "pub mod modifications;"
+ 2 ""
+ 3 "pub use data::{content_set, Blob, Content, ContentMutRef, Tree};"
| .%2Fsrc%2Fcore%2Fobjects%2Fdata blob.rs
+ 0 "use std::{fs, path::PathBuf};"
+ 1 ""
+ 2 "use serde::{Deserialize, Serialize};"
+ 3 ""
+ 4 "use crate::{core::modifications, error::RelicError};"
+ 5 ""
+ 6 "#[derive(Debug, Serialize, Deserialize, Clone)]"
+ 7 "pub struct Blob {"
+ 8 "    pub name: String,"
+ 9 "    pub content: String,"
+ 10 "}"
+ 11 ""
+ 12 "impl Blob {"
+ 13 "    pub fn new() -> Blob {"
+ 14 "        Blob {"
+ 15 "            name: \"\".to_string(),"
+ 16 "            content: \"\".to_string(),"
+ 17 "        }"
+ 18 "    }"
+ 19 ""
+ 20 "    pub fn create(name: String, path: PathBuf) -> Result<Blob, RelicError> {"
+ 21 "        match fs::read_to_string(path) {"
+ 22 "            Ok(content) => Ok(Blob {"
+ 23 "                name: name,"
+ 24 "                content: content,"
+ 25 "            }),"
+ 26 "            Err(_) => Err(RelicError::FileCantOpen),"
+ 27 "        }"
+ 28 "    }"
+ 29 ""
+ 30 "    pub fn apply_changes(&mut self, modifications: &Vec<modifications::Blob>) {"
+ 31 "        // TODO : investigate whether an additional newline is added to eof"
+ 32 "        // BUG : when the file has only one line, diffs start to break"
+ 33 "        //"
+ 34 "        // content : \"\""
+ 35 "        // Create(,, 0, \"something\")"
+ 36 "        // result : \"something\\nsomething\""
+ 37 "        //"
+ 38 "        // content : \"something\\nsomething\""
+ 39 "        // Delete(,, 0)"
+ 40 "        // result : \"\""
+ 41 ""
+ 42 "        // CHANGES ARE BEING APPLIED TO THE WRONG FILE"
+ 43 "        // APPLY CHANGES TO UPSTREAM, NOT CURRENT"
+ 44 ""
+ 45 "        // TODO : revise modification order"
+ 46 ""
+ 47 "        // deletions first then creations?"
+ 48 "        //      sorted largest to smallest"
+ 49 "        // creations sorted smallest to largest?"
+ 50 "        let mut lines = self"
+ 51 "            .content"
+ 52 "            .split(\"\\n\")"
+ 53 "            .map(|x| x.to_string())"
+ 54 "            .collect::<Vec<String>>();"
+ 55 ""
+ 56 "        let mut modifications = modifications.clone();"
+ 57 "        modifications.sort_by_key(|m| match m {"
+ 58 "            modifications::Blob::Create(_, _, l, _) => *l as i128,"
+ 59 "            modifications::Blob::Delete(_, _, l, _) => -(*l as i128),"
+ 60 "        });"
+ 61 ""
+ 62 "        for m in &modifications {"
+ 63 "            match m {"
+ 64 "                modifications::Blob::Create(_, _, line, content) => {"
+ 65 "                    // insert at that line"
+ 66 "                    lines.insert(*line, content.clone());"
+ 67 "                }"
+ 68 "                modifications::Blob::Delete(_, _, line, _) => {"
+ 69 "                    // delete that line"
+ 70 "                    lines.remove(*line);"
+ 71 "                }"
+ 72 "            }"
+ 73 "        }"
+ 74 ""
+ 75 "        self.content = lines.join(\"\\n\");"
+ 76 "    }"
+ 77 "}"
| .%2Fsrc%2Fcore%2Fobjects%2Fdata content.rs
+ 0 "use serde::{Deserialize, Serialize};"
+ 1 ""
+ 2 "use crate::core::{Blob, Tree};"
+ 3 ""
+ 4 "#[derive(Debug, Serialize, Deserialize, Clone)]"
+ 5 "pub enum Content {"
+ 6 "    Tree(Tree),"
+ 7 "    Blob(Blob),"
+ 8 "}"
+ 9 ""
+ 10 "#[derive(Debug)]"
+ 11 "pub enum ContentMutRef<'a> {"
+ 12 "    Tree(&'a mut Tree),"
+ 13 "    Blob(&'a mut Blob),"
+ 14 "}"
| .%2Fsrc%2Fcore%2Fobjects%2Fdata content_set.rs
+ 0 "use std::{"
+ 1 "    collections::HashSet,"
+ 2 "    path::PathBuf,"
+ 3 "    sync::{Arc, Mutex},"
+ 4 "};"
+ 5 ""
+ 6 "use serde::{Deserialize, Serialize};"
+ 7 ""
+ 8 "use crate::core::{ContentMutRef, Tree};"
+ 9 ""
+ 10 "pub const DEFAULT_IGNORE: &str = r#\"-- Added by Relic: Automatically ignore all git content"
+ 11 ".git/"
+ 12 ".gitignore\"#;"
+ 13 ""
+ 14 "#[derive(Serialize, Deserialize, Debug, Clone)]"
+ 15 "pub struct ContentSet {"
+ 16 "    pub directories: HashSet<String>,"
+ 17 "    pub files: HashSet<String>,"
+ 18 "}"
+ 19 "impl ContentSet {"
+ 20 "    pub fn empty() -> ContentSet {"
+ 21 "        ContentSet {"
+ 22 "            directories: HashSet::new(),"
+ 23 "            files: HashSet::new(),"
+ 24 "        }"
+ 25 "    }"
+ 26 ""
+ 27 "    pub fn as_set(&self) -> HashSet<String> {"
+ 28 "        HashSet::new()"
+ 29 "    }"
+ 30 "}"
+ 31 ""
+ 32 "pub trait IgnoreSet {"
+ 33 "    fn create(content: String) -> Self;"
+ 34 "}"
+ 35 "impl IgnoreSet for ContentSet {"
+ 36 "    fn create(content: String) -> ContentSet {"
+ 37 "        let mut result = ContentSet {"
+ 38 "            directories: HashSet::new(),"
+ 39 "            files: HashSet::new(),"
+ 40 "        };"
+ 41 ""
+ 42 "        // always ignore the .relic directory"
+ 43 "        result.directories.insert(\".relic\".to_string());"
+ 44 ""
+ 45 "        for line in content.split(\"\\n\") {"
+ 46 "            if line.is_empty() {"
+ 47 "                continue;"
+ 48 "            }"
+ 49 ""
+ 50 "            // skip comments"
+ 51 "            if line.starts_with(\"-- \") {"
+ 52 "                continue;"
+ 53 "            }"
+ 54 ""
+ 55 "            // doesnt take into account cases like"
+ 56 "            // some_directory// <- double slashes"
+ 57 "            if line.ends_with(\"/\") {"
+ 58 "                let i = line[0..line.len() - 1].to_string();"
+ 59 "                if i.is_empty() {"
+ 60 "                    continue;"
+ 61 "                }"
+ 62 ""
+ 63 "                result.directories.insert(i);"
+ 64 "            } else {"
+ 65 "                result.files.insert(line.to_string());"
+ 66 "            }"
+ 67 "        }"
+ 68 ""
+ 69 "        result"
+ 70 "    }"
+ 71 "}"
+ 72 ""
+ 73 "pub trait TrackingSet {"
+ 74 "    fn deserialise(content: String) -> Self;"
+ 75 "    fn initialise(&self, d: &mut Tree) -> Self;"
+ 76 "}"
+ 77 "impl TrackingSet for ContentSet {"
+ 78 "    fn deserialise(content: String) -> Self {"
+ 79 "        let mut result = ContentSet::empty();"
+ 80 ""
+ 81 "        for d in content"
+ 82 "            .split(\"\\n\")"
+ 83 "            .map(|x| x.to_string())"
+ 84 "            .collect::<Vec<String>>()"
+ 85 "        {"
+ 86 "            if d.ends_with(\"/\") {"
+ 87 "                // dir"
+ 88 "                result.directories.insert(d[..d.len() - 1].to_string());"
+ 89 "            } else {"
+ 90 "                // file"
+ 91 "                result.files.insert(d);"
+ 92 "            }"
+ 93 "        }"
+ 94 ""
+ 95 "        result"
+ 96 "    }"
+ 97 ""
+ 98 "    fn initialise(&self, d: &mut Tree) -> ContentSet {"
+ 99 "        let tracked_mutex = Arc::new(Mutex::new(self.clone()));"
+ 100 "        d.traverse("
+ 101 "            PathBuf::from(\".\"),"
+ 102 "            &|path, _, current| {"
+ 103 "                // println!(\"traversing at : {path:?}\");"
+ 104 ""
+ 105 "                let mut tracked_unlock = tracked_mutex.lock().unwrap();"
+ 106 ""
+ 107 "                match current {"
+ 108 "                    ContentMutRef::Tree(t) => {"
+ 109 "                        // if parent in set"
+ 110 "                        // add to content set"
+ 111 "                        if tracked_unlock"
+ 112 "                            .directories"
+ 113 "                            .contains(&t.path.parent().unwrap().to_string_lossy().to_string())"
+ 114 "                        {"
+ 115 "                            tracked_unlock"
+ 116 "                                .directories"
+ 117 "                                .insert(t.path.to_string_lossy().to_string());"
+ 118 "                        }"
+ 119 "                    }"
+ 120 "                    ContentMutRef::Blob(b) => {"
+ 121 "                        if tracked_unlock"
+ 122 "                            .directories"
+ 123 "                            .contains(&path.to_string_lossy().to_string())"
+ 124 "                        {"
+ 125 "                            tracked_unlock"
+ 126 "                                .files"
+ 127 "                                .insert(path.join(&b.name).to_string_lossy().to_string());"
+ 128 "                        }"
+ 129 "                    }"
+ 130 "                }"
+ 131 "            },"
+ 132 "            &d.clone(),"
+ 133 "        );"
+ 134 ""
+ 135 "        // dont ask me"
+ 136 "        let result = tracked_mutex.lock().unwrap().clone();"
+ 137 "        result"
+ 138 "    }"
+ 139 "}"
| .%2Fsrc%2Fcore%2Fobjects%2Fdata mod.rs
+ 0 "pub mod blob;"
+ 1 "pub mod content;"
+ 2 "pub mod content_set;"
+ 3 "pub mod tree;"
+ 4 "pub mod upstream;"
+ 5 ""
+ 6 "pub use blob::Blob;"
+ 7 "pub use tree::Tree;"
+ 8 ""
+ 9 "pub use content::{Content, ContentMutRef};"
+ 10 "pub use upstream::Upstream;"
| .%2Fsrc%2Fcore%2Fobjects%2Fdata tree.rs
+ 0 "use std::{"
+ 1 "    collections::{HashMap, HashSet},"
+ 2 "    path::PathBuf,"
+ 3 "    sync::{Arc, Mutex},"
+ 4 "};"
+ 5 ""
+ 6 "use serde::{Deserialize, Serialize};"
+ 7 ""
+ 8 "use crate::core::{"
+ 9 "    modifications::{self, Change},"
+ 10 "    Blob, Content, ContentMutRef,"
+ 11 "};"
+ 12 ""
+ 13 "#[derive(Debug, Serialize, Deserialize, Clone)]"
+ 14 "pub struct Tree {"
+ 15 "    pub path: PathBuf,"
+ 16 "    pub name: String,"
+ 17 "    pub content: Vec<Content>,"
+ 18 "}"
+ 19 ""
+ 20 "impl Tree {"
+ 21 "    pub fn new() -> Tree {"
+ 22 "        Tree {"
+ 23 "            path: PathBuf::from(\".\"),"
+ 24 "            name: \"\".to_string(),"
+ 25 "            content: vec![],"
+ 26 "        }"
+ 27 "    }"
+ 28 ""
+ 29 "    pub fn get_hash(&self) -> String {"
+ 30 "        sha256::digest(serde_json::to_string(&self).unwrap())"
+ 31 "    }"
+ 32 ""
+ 33 "    pub fn deserialise(s: String) -> Option<Tree> {"
+ 34 "        match serde_json::from_str(&s) {"
+ 35 "            Ok(d) => Some(d),"
+ 36 "            _ => None,"
+ 37 "        }"
+ 38 "    }"
+ 39 ""
+ 40 "    pub fn deserialise_content(s: String) -> Option<Vec<Content>> {"
+ 41 "        // used only for init"
+ 42 "        // same as deserialise, but only content, no name or path"
+ 43 "        match serde_json::from_str(&s) {"
+ 44 "            Ok(d) => Some(d),"
+ 45 "            _ => None,"
+ 46 "        }"
+ 47 "    }"
+ 48 ""
+ 49 "    pub fn serialise(&self) -> String {"
+ 50 "        serde_json::to_string_pretty(&self).unwrap()"
+ 51 "    }"
+ 52 ""
+ 53 "    pub fn apply_changes(&mut self, changes: Change) {"
+ 54 "        let (c_mod_map, mod_map) = changes.as_map();"
+ 55 "        let c_mod_map = Arc::new(Mutex::new(c_mod_map));"
+ 56 ""
+ 57 "        // two pass"
+ 58 "        // create/delete containers, then create/delete file content"
+ 59 ""
+ 60 "        self.traverse("
+ 61 "            PathBuf::from(\".\"),"
+ 62 "            &|_, _, current| {"
+ 63 "                if let ContentMutRef::Tree(t) = current {"
+ 64 "                    // somehow denote that the parent does not yet exist,"
+ 65 "                    // possibly recursively create trees where needed"
+ 66 ""
+ 67 "                    // TODO : optimise the match arms"
+ 68 "                    let mut c_mod_map_lock = c_mod_map.lock().unwrap();"
+ 69 "                    if let Some(c_modifications) ="
+ 70 "                        c_mod_map_lock.get(&t.path.to_string_lossy().to_string())"
+ 71 "                    {"
+ 72 "                        let c_clone = c_modifications.clone();"
+ 73 ""
+ 74 "                        // deals with additions"
+ 75 "                        t.content.append(&mut recursive_birth("
+ 76 "                            &PathBuf::from(t.path.clone()),"
+ 77 "                            &mut c_mod_map_lock,"
+ 78 "                        ));"
+ 79 ""
+ 80 "                        let mut deleted_containers = HashSet::new();"
+ 81 "                        // deals with subtractions"
+ 82 "                        for c_mod in &c_clone {"
+ 83 "                            match c_mod {"
+ 84 "                                modifications::Tree::DeleteTree(_, n) => {"
+ 85 "                                    deleted_containers.insert(n);"
+ 86 "                                }"
+ 87 "                                modifications::Tree::DeleteBlob(_, n) => {"
+ 88 "                                    deleted_containers.insert(n);"
+ 89 "                                }"
+ 90 "                                _ => {}"
+ 91 "                            }"
+ 92 "                        }"
+ 93 ""
+ 94 "                        t.content = t"
+ 95 "                            .content"
+ 96 "                            .iter()"
+ 97 "                            .filter(|x| {"
+ 98 "                                !deleted_containers.contains(match x {"
+ 99 "                                    Content::Blob(b) => &b.name,"
+ 100 "                                    Content::Tree(t) => &t.name,"
+ 101 "                                })"
+ 102 "                            })"
+ 103 "                            .map(|x| x.clone())"
+ 104 "                            .collect::<Vec<Content>>();"
+ 105 "                    }"
+ 106 "                }"
+ 107 "            },"
+ 108 "            &Tree::new(),"
+ 109 "        );"
+ 110 ""
+ 111 "        self.traverse("
+ 112 "            PathBuf::from(\".\"),"
+ 113 "            &|path, _, current| {"
+ 114 "                if let ContentMutRef::Blob(f) = current {"
+ 115 "                    if let Some(modifications) = mod_map"
+ 116 "                        .get(&path.to_string_lossy().to_string())"
+ 117 "                        .map_or(None, |x| x.get(&f.name))"
+ 118 "                    {"
+ 119 "                        f.apply_changes(modifications);"
+ 120 "                    }"
+ 121 "                }"
+ 122 "            },"
+ 123 "            &self.clone(),"
+ 124 "        );"
+ 125 ""
+ 126 "        pub fn recursive_birth("
+ 127 "            parent_directory: &PathBuf,"
+ 128 "            c_mod_map: &mut HashMap<String, HashSet<modifications::Tree>>,"
+ 129 "        ) -> Vec<Content> {"
+ 130 "            // pass the new directory's parent directory"
+ 131 "            let mut result = vec![];"
+ 132 "            if let Some(c_modifications) ="
+ 133 "                c_mod_map.get_mut(&parent_directory.to_string_lossy().to_string())"
+ 134 "            {"
+ 135 "                let c_clone = c_modifications.clone();"
+ 136 "                for c in &c_clone {"
+ 137 "                    c_modifications.remove(&c);"
+ 138 "                }"
+ 139 "                for c_mod in c_clone {"
+ 140 "                    match c_mod {"
+ 141 "                        modifications::Tree::CreateTree(_, n) => {"
+ 142 "                            result.push(Content::Tree(Tree {"
+ 143 "                                path: parent_directory.join(n.clone()),"
+ 144 "                                name: n.clone(),"
+ 145 "                                content: recursive_birth("
+ 146 "                                    &parent_directory.join(n.clone()),"
+ 147 "                                    c_mod_map,"
+ 148 "                                ),"
+ 149 "                            }));"
+ 150 "                        }"
+ 151 "                        modifications::Tree::CreateBlob(_, n) => result.push(Content::Blob(Blob {"
+ 152 "                            name: n.clone(),"
+ 153 "                            content: \"\".to_string(),"
+ 154 "                        })),"
+ 155 "                        _ => {}"
+ 156 "                    }"
+ 157 "                }"
+ 158 "            }"
+ 159 "            result"
+ 160 "        }"
+ 161 "    }"
+ 162 ""
+ 163 "    pub fn unapply_changes(&mut self, changes: Change) {"
+ 164 "        // TODO : test if 100% reliable"
+ 165 "        let changes = changes.inverse();"
+ 166 "        self.apply_changes(changes);"
+ 167 "        // TODO : update upstream?"
+ 168 "    }"
+ 169 ""
+ 170 "    pub fn traverse<F>(&mut self, root_path: PathBuf, func: &F, parent: &Tree)"
+ 171 "    where"
+ 172 "        // parent path, parent tree, current content"
+ 173 "        F: Fn(&PathBuf, &Tree, ContentMutRef),"
+ 174 "    {"
+ 175 "        func(&root_path, &parent, ContentMutRef::Tree(self));"
+ 176 ""
+ 177 "        let c = self.clone();"
+ 178 "        for content in &mut self.content {"
+ 179 "            match content {"
+ 180 "                Content::Tree(t) => {"
+ 181 "                    t.traverse(root_path.join(t.name.clone()), func, &c);"
+ 182 "                }"
+ 183 "                Content::Blob(b) => {"
+ 184 "                    func(&root_path, &c, ContentMutRef::Blob(b));"
+ 185 "                }"
+ 186 "            }"
+ 187 "        }"
+ 188 "    }"
+ 189 "}"
| .%2Fsrc%2Fcore%2Fobjects%2Fdata upstream.rs
+ 0 "// upstream might be stored in differing formats"
+ 1 "// this is here to ensure backwards compatibility with outdated standards and etc"
+ 2 ""
+ 3 "use std::{fs, path::PathBuf};"
+ 4 ""
+ 5 "use serde::{Deserialize, Serialize};"
+ 6 ""
+ 7 "use crate::{"
+ 8 "    core::{Content, Tree},"
+ 9 "    error::RelicError,"
+ 10 "};"
+ 11 ""
+ 12 "#[derive(Serialize, Deserialize, Debug, Clone)]"
+ 13 "pub struct Upstream {"
+ 14 "    pub convention: String, // used for backwards compatibility"
+ 15 "    pub content: Vec<Content>,"
+ 16 "}"
+ 17 "impl Upstream {"
+ 18 "    pub fn empty() -> Upstream {"
+ 19 "        Upstream {"
+ 20 "            convention: \"0.0.1\".to_string(),"
+ 21 "            content: vec![],"
+ 22 "        }"
+ 23 "    }"
+ 24 ""
+ 25 "    pub fn tree(self) -> Tree {"
+ 26 "        Tree {"
+ 27 "            path: PathBuf::from(\".\"),"
+ 28 "            name: \"\".to_string(),"
+ 29 "            content: self.content,"
+ 30 "        }"
+ 31 "    }"
+ 32 ""
+ 33 "    pub fn from_tree(tree: &Tree) -> Upstream {"
+ 34 "        Upstream {"
+ 35 "            convention: \"0.0.1\".to_string(),"
+ 36 "            content: tree.content.clone(),"
+ 37 "        }"
+ 38 "    }"
+ 39 ""
+ 40 "    pub fn serialise(&self) -> String {"
+ 41 "        serde_json::to_string(&self).unwrap()"
+ 42 "    }"
+ 43 ""
+ 44 "    pub fn deserialise(path: &str) -> Result<Upstream, RelicError> {"
+ 45 "        match fs::read_to_string(path) {"
+ 46 "            Ok(data) => match serde_json::from_str::<Upstream>(&data) {"
+ 47 "                Ok(u) => Ok(u),"
+ 48 "                // TODO: implement backwards compatibility"
+ 49 "                Err(_) => Err(RelicError::ConfigurationIncorrect),"
+ 50 "            },"
+ 51 "            Err(_) => Err(RelicError::FileCantOpen),"
+ 52 "        }"
+ 53 "    }"
+ 54 "}"
| .%2Fsrc%2Fcore%2Fobjects%2Fmodifications blob.rs
+ 0 "use serde::{Deserialize, Serialize};"
+ 1 ""
+ 2 "#[derive(Debug, Clone, Serialize, Deserialize, Hash, PartialEq, Eq, PartialOrd, Ord)]"
+ 3 "pub enum Blob {"
+ 4 "    // creation/deletion of lines in files"
+ 5 "    Create("
+ 6 "        String, // parent directory"
+ 7 "        String, // file name"
+ 8 "        usize,  // line"
+ 9 "        String, // text"
+ 10 "    ),"
+ 11 "    Delete("
+ 12 "        String, // parent directory"
+ 13 "        String, // file name"
+ 14 "        usize,  // line"
+ 15 "        String, // text"
+ 16 "    ),"
+ 17 "}"
+ 18 ""
+ 19 "impl Blob {"
+ 20 "    pub fn extract_path(&self) -> (String, String) {"
+ 21 "        match self {"
+ 22 "            Blob::Create(path, name, _, _) | Blob::Delete(path, name, _, _) => {"
+ 23 "                (path.clone(), name.clone())"
+ 24 "            }"
+ 25 "        }"
+ 26 "    }"
+ 27 ""
+ 28 "    pub fn extract_change(&self) -> String {"
+ 29 "        format!("
+ 30 "            \"{} {}\","
+ 31 "            match self {"
+ 32 "                Blob::Create(_, _, _, _) => \"+\","
+ 33 "                Blob::Delete(_, _, _, _) => \"-\","
+ 34 "            },"
+ 35 "            match self {"
+ 36 "                Blob::Create(_, _, line, content) | Blob::Delete(_, _, line, content) => {"
+ 37 "                    format!(\"{line} {content:?}\")"
+ 38 "                }"
+ 39 "            }"
+ 40 "        )"
+ 41 "    }"
+ 42 "}"
| .%2Fsrc%2Fcore%2Fobjects%2Fmodifications container.rs
+ 0 "use serde::{Deserialize, Serialize};"
+ 1 ""
+ 2 "#[derive(Debug, Clone, Serialize, Deserialize, Hash, PartialEq, Eq, PartialOrd, Ord)]"
+ 3 "pub enum Tree {"
+ 4 "    // denote that parent doesnt exist?"
+ 5 ""
+ 6 "    // creation/deletion of files & folders"
+ 7 "    CreateTree("
+ 8 "        String, // parent directory"
+ 9 "        String, // name"
+ 10 "    ),"
+ 11 "    DeleteTree("
+ 12 "        String, // parent directory"
+ 13 "        String, // name"
+ 14 "    ),"
+ 15 ""
+ 16 "    CreateBlob("
+ 17 "        String, // parent directory"
+ 18 "        String, // name"
+ 19 "    ),"
+ 20 "    DeleteBlob("
+ 21 "        String, // parent directory"
+ 22 "        String, // name"
+ 23 "    ),"
+ 24 "}"
+ 25 "impl Tree {"
+ 26 "    pub fn extract_data(&self) -> (String, String) {"
+ 27 "        match self {"
+ 28 "            Tree::CreateTree(path, name)"
+ 29 "            | Tree::DeleteTree(path, name)"
+ 30 "            | Tree::CreateBlob(path, name)"
+ 31 "            | Tree::DeleteBlob(path, name) => (path.clone(), name.clone()),"
+ 32 "        }"
+ 33 "    }"
+ 34 ""
+ 35 "    pub fn serialise(&self) -> String {"
+ 36 "        format!("
+ 37 "            \"{} {}\","
+ 38 "            match self {"
+ 39 "                Tree::CreateTree(_, _) => {"
+ 40 "                    \"+ D\""
+ 41 "                }"
+ 42 "                Tree::DeleteTree(_, _) => {"
+ 43 "                    \"- D\""
+ 44 "                }"
+ 45 "                Tree::CreateBlob(_, _) => {"
+ 46 "                    \"+ F\""
+ 47 "                }"
+ 48 "                Tree::DeleteBlob(_, _) => {"
+ 49 "                    \"- F\""
+ 50 "                }"
+ 51 "            },"
+ 52 "            match self {"
+ 53 "                Tree::CreateTree(p, n)"
+ 54 "                | Tree::DeleteTree(p, n)"
+ 55 "                | Tree::CreateBlob(p, n)"
+ 56 "                | Tree::DeleteBlob(p, n) => {"
+ 57 "                    format!("
+ 58 "                        \"{} {}\","
+ 59 "                        urlencoding::encode(&p).to_string(),"
+ 60 "                        urlencoding::encode(&n).to_string()"
+ 61 "                    )"
+ 62 "                }"
+ 63 "            }"
+ 64 "        )"
+ 65 "    }"
+ 66 "}"
| .%2Fsrc%2Fcore%2Fobjects%2Fmodifications mod.rs
+ 0 "pub mod change;"
+ 1 "pub mod container;"
+ 2 "pub mod blob;"
+ 3 ""
+ 4 "pub use change::Change;"
+ 5 "pub use container::Tree;"
+ 6 "pub use blob::Blob;"
| .%2Fsrc%2Fcore%2Fobjects%2Fmodifications%2Fchange constructor.rs
+ 0 "use std::{"
+ 1 "    collections::{HashMap, HashSet},"
+ 2 "    path::Path,"
+ 3 "};"
+ 4 ""
+ 5 "use similar::{ChangeTag, TextDiff};"
+ 6 ""
+ 7 "use crate::core::{modifications, Blob, Content, Tree};"
+ 8 ""
+ 9 "use super::Change;"
+ 10 ""
+ 11 "impl Change {"
+ 12 "    pub fn get_change("
+ 13 "        path: String,"
+ 14 "        upstream_blob: &Blob,"
+ 15 "        current_blob: &Blob,"
+ 16 "    ) -> Vec<modifications::Blob> {"
+ 17 "        // https://blog.jcoglan.com/2017/02/15/the-myers-diff-algorithm-part-2/"
+ 18 "        // for our change algorithm, we will be using myers diff algorithm"
+ 19 "        // basically a shortest distance problem, with downwards, rightwards and diagonal directions as movement choices"
+ 20 "        // (note that diagonal movements do not contribute towards the distance)"
+ 21 ""
+ 22 "        // similar does not handle newlines at eof well at all"
+ 23 "        // this is the workaround for it"
+ 24 "        let upstream = format!(\"{}\\n\", upstream_blob.content.clone());"
+ 25 "        let current = format!(\"{}\\n\", current_blob.content.clone());"
+ 26 ""
+ 27 "        // TODO : compare hashes instead of blobs"
+ 28 "        if upstream == current {"
+ 29 "            return vec![];"
+ 30 "        }"
+ 31 ""
+ 32 "        let mut result = vec![];"
+ 33 "        let diff = TextDiff::from_lines(&upstream, &current);"
+ 34 ""
+ 35 "        for change in diff.iter_all_changes().filter_map(|c| match c.tag() {"
+ 36 "            ChangeTag::Equal => None,"
+ 37 "            _ => Some(c),"
+ 38 "        }) {"
+ 39 "            result.push(match change.tag() {"
+ 40 "                ChangeTag::Delete => modifications::Blob::Delete("
+ 41 "                    path.clone(),"
+ 42 "                    current_blob.name.clone(),"
+ 43 "                    change.old_index().unwrap(),"
+ 44 "                    change.to_string().strip_suffix(\"\\n\").unwrap().to_string(),"
+ 45 "                ),"
+ 46 "                ChangeTag::Insert => modifications::Blob::Create("
+ 47 "                    path.clone(),"
+ 48 "                    current_blob.name.clone(),"
+ 49 "                    change.new_index().unwrap(),"
+ 50 "                    change.to_string().strip_suffix(\"\\n\").unwrap().to_string(),"
+ 51 "                ),"
+ 52 "                _ => panic!(\"Unmatched change type: {}\", change),"
+ 53 "            })"
+ 54 "        }"
+ 55 ""
+ 56 "        result"
+ 57 "    }"
+ 58 ""
+ 59 "    pub fn get_change_all(upstream: &Tree, current: &Tree, path: &Path) -> Change {"
+ 60 "        // assume that both current and previous have the same tree names"
+ 61 "        // has to be bfs"
+ 62 ""
+ 63 "        // initialise current state set"
+ 64 "        let mut current_set = HashSet::new();"
+ 65 "        let mut current_map = HashMap::new();"
+ 66 "        for c in &current.content {"
+ 67 "            match c {"
+ 68 "                Content::Tree(t) => {"
+ 69 "                    current_set.insert((t.name.clone(), false));"
+ 70 "                    current_map.insert((t.name.clone(), false), c);"
+ 71 "                }"
+ 72 "                Content::Blob(b) => {"
+ 73 "                    current_set.insert((b.name.clone(), true));"
+ 74 "                    current_map.insert((b.name.clone(), true), c);"
+ 75 "                }"
+ 76 "            }"
+ 77 "        }"
+ 78 "        //"
+ 79 ""
+ 80 "        // initialise upstream state set"
+ 81 "        let mut upstream_set = HashSet::new();"
+ 82 "        let mut upstream_map = HashMap::new();"
+ 83 "        for c in &upstream.content {"
+ 84 "            match c {"
+ 85 "                Content::Tree(t) => {"
+ 86 "                    upstream_set.insert((t.name.clone(), false));"
+ 87 "                    upstream_map.insert((t.name.clone(), false), c);"
+ 88 "                }"
+ 89 "                Content::Blob(b) => {"
+ 90 "                    upstream_set.insert((b.name.clone(), true));"
+ 91 "                    upstream_map.insert((b.name.clone(), true), c);"
+ 92 "                }"
+ 93 "            }"
+ 94 "        }"
+ 95 "        //"
+ 96 ""
+ 97 "        // use set differences to determine blob and tree creation or deletion"
+ 98 "        let deleted = upstream_set"
+ 99 "            .difference(&current_set)"
+ 100 "            .map(|(n, t)| (n.to_string(), *t))"
+ 101 "            .collect::<Vec<(String, bool)>>();"
+ 102 "        let created = current_set"
+ 103 "            .difference(&upstream_set)"
+ 104 "            .map(|(n, t)| (n.to_string(), *t))"
+ 105 "            .collect::<Vec<(String, bool)>>();"
+ 106 "        //"
+ 107 ""
+ 108 "        // for all deleted blobs, log them"
+ 109 "        // for all deleted trees, log them and do the same for all children"
+ 110 "        let mut container_modifications = vec![];"
+ 111 "        let mut modifications = vec![];"
+ 112 "        for (name, is_blob) in deleted {"
+ 113 "            if is_blob {"
+ 114 "                container_modifications.push(modifications::Tree::DeleteBlob("
+ 115 "                    path.to_string_lossy().to_string(),"
+ 116 "                    name,"
+ 117 "                ));"
+ 118 "            } else {"
+ 119 "                container_modifications.push(modifications::Tree::DeleteTree("
+ 120 "                    path.to_string_lossy().to_string(),"
+ 121 "                    name.clone(),"
+ 122 "                ));"
+ 123 "                // traverse all children, add them to result as well"
+ 124 "                let mut changes = Change::get_change_all("
+ 125 "                    match upstream_map.get(&(name.clone(), false)).unwrap() {"
+ 126 "                        Content::Tree(deleted_tree) => deleted_tree,"
+ 127 "                        _ => panic!(),"
+ 128 "                    },"
+ 129 "                    &Tree::new(),"
+ 130 "                    &path.join(name.clone()),"
+ 131 "                );"
+ 132 "                container_modifications.append(&mut changes.trees);"
+ 133 "                modifications.append(&mut changes.blobs);"
+ 134 "            }"
+ 135 "        }"
+ 136 "        //"
+ 137 ""
+ 138 "        // for all created blobs, log them"
+ 139 "        // for all created trees, log them and do the same for all children"
+ 140 "        for (name, is_blob) in created {"
+ 141 "            if is_blob {"
+ 142 "                container_modifications.push(modifications::Tree::CreateBlob("
+ 143 "                    path.to_string_lossy().to_string(),"
+ 144 "                    name.clone(),"
+ 145 "                ));"
+ 146 "                modifications.append(&mut Change::get_change("
+ 147 "                    path.to_string_lossy().to_string(),"
+ 148 "                    &Blob::new(),"
+ 149 "                    match current_map.get(&(name, true)).unwrap() {"
+ 150 "                        Content::Blob(b) => b,"
+ 151 "                        _ => panic!(),"
+ 152 "                    },"
+ 153 "                ))"
+ 154 "            } else {"
+ 155 "                container_modifications.push(modifications::Tree::CreateTree("
+ 156 "                    path.to_string_lossy().to_string(),"
+ 157 "                    name.clone(),"
+ 158 "                ));"
+ 159 ""
+ 160 "                let mut changes = Change::get_change_all("
+ 161 "                    &Tree::new(),"
+ 162 "                    match current_map.get(&(name.clone(), false)).unwrap() {"
+ 163 "                        Content::Tree(t) => t,"
+ 164 "                        _ => panic!(),"
+ 165 "                    },"
+ 166 "                    &path.join(name.clone()),"
+ 167 "                );"
+ 168 "                container_modifications.append(&mut changes.trees);"
+ 169 "                modifications.append(&mut changes.blobs);"
+ 170 "            }"
+ 171 "        }"
+ 172 ""
+ 173 "        for content in &current.content {"
+ 174 "            match content {"
+ 175 "                Content::Tree(tree) => {"
+ 176 "                    // get the matching upstream tree"
+ 177 "                    // if it doesnt exist, that means the content is new and can be ignored"
+ 178 "                    // we ignore it because we have already logged it in the section above"
+ 179 "                    let p = path.join(tree.name.clone());"
+ 180 "                    let upstream_tree = match upstream_map.get(&(tree.name.clone(), false)) {"
+ 181 "                        Some(u) => match u {"
+ 182 "                            Content::Tree(u_t) => u_t,"
+ 183 "                            _ => panic!(),"
+ 184 "                        },"
+ 185 "                        _ => {"
+ 186 "                            continue;"
+ 187 "                        }"
+ 188 "                    };"
+ 189 "                    //"
+ 190 ""
+ 191 "                    let mut changes = Change::get_change_all(upstream_tree, tree, &p);"
+ 192 "                    container_modifications.append(&mut changes.trees);"
+ 193 "                    modifications.append(&mut changes.blobs);"
+ 194 "                }"
+ 195 "                Content::Blob(b) => {"
+ 196 "                    let upstream_blob = match upstream_map.get(&(b.name.clone(), true)) {"
+ 197 "                        Some(c) => match c {"
+ 198 "                            Content::Blob(b) => b,"
+ 199 "                            _ => panic!(),"
+ 200 "                        },"
+ 201 "                        None => {"
+ 202 "                            continue;"
+ 203 "                        }"
+ 204 "                    };"
+ 205 ""
+ 206 "                    modifications.append(&mut Change::get_change("
+ 207 "                        path.to_string_lossy().to_string(),"
+ 208 "                        &upstream_blob,"
+ 209 "                        &b,"
+ 210 "                    ));"
+ 211 "                }"
+ 212 "            }"
+ 213 "        }"
+ 214 ""
+ 215 "        Change {"
+ 216 "            trees: container_modifications,"
+ 217 "            blobs: modifications,"
+ 218 "        }"
+ 219 "    }"
+ 220 "}"
| .%2Fsrc%2Fcore%2Fobjects%2Fmodifications%2Fchange filter.rs
+ 0 "use std::path::PathBuf;"
+ 1 ""
+ 2 "use crate::core::{content_set::ContentSet, modifications};"
+ 3 ""
+ 4 "use super::Change;"
+ 5 ""
+ 6 "impl Change {"
+ 7 "    pub fn filter_changes(&self, filter: &ContentSet) -> Change {"
+ 8 "        Change {"
+ 9 "            trees: self"
+ 10 "                .trees"
+ 11 "                .clone()"
+ 12 "                .into_iter()"
+ 13 "                .filter(|c_mod| match c_mod {"
+ 14 "                    modifications::Tree::CreateBlob(p, n)"
+ 15 "                    | modifications::Tree::DeleteBlob(p, n) => filter"
+ 16 "                        .files"
+ 17 "                        .contains(&PathBuf::from(p).join(n).to_string_lossy().to_string()),"
+ 18 "                    modifications::Tree::CreateTree(p, n)"
+ 19 "                    | modifications::Tree::DeleteTree(p, n) => filter"
+ 20 "                        .directories"
+ 21 "                        .contains(&PathBuf::from(p).join(n).to_string_lossy().to_string()),"
+ 22 "                })"
+ 23 "                .collect(),"
+ 24 "            blobs: self"
+ 25 "                .blobs"
+ 26 "                .clone()"
+ 27 "                .into_iter()"
+ 28 "                .filter(|m| {"
+ 29 "                    // if only can map a tuple"
+ 30 "                    filter.files.contains(&match m {"
+ 31 "                        modifications::Blob::Create(p, n, _, _)"
+ 32 "                        | modifications::Blob::Delete(p, n, _, _) => {"
+ 33 "                            PathBuf::from(p).join(n).to_string_lossy().to_string()"
+ 34 "                        }"
+ 35 "                    })"
+ 36 "                })"
+ 37 "                .collect(),"
+ 38 "        }"
+ 39 "    }"
+ 40 "}"
| .%2Fsrc%2Fcore%2Fobjects%2Fmodifications%2Fchange inverse.rs
+ 0 "use crate::core::modifications;"
+ 1 ""
+ 2 "use super::Change;"
+ 3 ""
+ 4 "impl Change {"
+ 5 "    pub fn inverse(&self) -> Change {"
+ 6 "        // TODO: test"
+ 7 ""
+ 8 "        // returns inverse of the change"
+ 9 "        // all additions are deletions and vice versa"
+ 10 ""
+ 11 "        // the order does not follow the optimised/intuitive format"
+ 12 "        // additions will appear before deletions if inversed"
+ 13 "        // but relic will always apply changes in the correct order regardless"
+ 14 ""
+ 15 "        Change {"
+ 16 "            trees: self"
+ 17 "                .trees"
+ 18 "                .iter()"
+ 19 "                .map(|c| match c {"
+ 20 "                    modifications::Tree::CreateBlob(p, n) => {"
+ 21 "                        modifications::Tree::DeleteBlob(p.to_string(), n.to_string())"
+ 22 "                    }"
+ 23 "                    modifications::Tree::CreateTree(p, n) => {"
+ 24 "                        modifications::Tree::DeleteTree(p.to_string(), n.to_string())"
+ 25 "                    }"
+ 26 "                    modifications::Tree::DeleteBlob(p, n) => {"
+ 27 "                        modifications::Tree::CreateBlob(p.to_string(), n.to_string())"
+ 28 "                    }"
+ 29 "                    modifications::Tree::DeleteTree(p, n) => {"
+ 30 "                        modifications::Tree::CreateTree(p.to_string(), n.to_string())"
+ 31 "                    }"
+ 32 "                })"
+ 33 "                .collect::<Vec<modifications::Tree>>(),"
+ 34 "            blobs: self"
+ 35 "                .blobs"
+ 36 "                .iter()"
+ 37 "                .map(|m| match m {"
+ 38 "                    modifications::Blob::Create(p, f, l, t) => {"
+ 39 "                        modifications::Blob::Delete(p.to_string(), f.to_string(), *l, t.to_string())"
+ 40 "                    }"
+ 41 "                    modifications::Blob::Delete(p, f, l, t) => {"
+ 42 "                        modifications::Blob::Create(p.to_string(), f.to_string(), *l, t.to_string())"
+ 43 "                    }"
+ 44 "                })"
+ 45 "                .collect::<Vec<modifications::Blob>>(),"
+ 46 "        }"
+ 47 "    }"
+ 48 "}"
| .%2Fsrc%2Fcore%2Fobjects%2Fmodifications%2Fchange mod.rs
+ 0 "mod constructor;"
+ 1 "mod filter;"
+ 2 "mod inverse;"
+ 3 "mod serialisation;"
+ 4 ""
+ 5 "use std::collections::{HashMap, HashSet};"
+ 6 ""
+ 7 "use serde::{Deserialize, Serialize};"
+ 8 ""
+ 9 "use crate::core::modifications;"
+ 10 ""
+ 11 "#[derive(Debug, Serialize, Deserialize, Clone)]"
+ 12 "pub struct Change {"
+ 13 "    pub trees: Vec<modifications::Tree>,"
+ 14 "    pub blobs: Vec<modifications::Blob>,"
+ 15 "}"
+ 16 "impl Change {"
+ 17 "    pub fn get_hash(&self) -> String {"
+ 18 "        sha256::digest(self.serialise_changes())"
+ 19 "    }"
+ 20 ""
+ 21 "    pub fn empty() -> Change {"
+ 22 "        Change {"
+ 23 "            trees: vec![],"
+ 24 "            blobs: vec![],"
+ 25 "        }"
+ 26 "    }"
+ 27 ""
+ 28 "    pub fn as_map("
+ 29 "        &self,"
+ 30 "    ) -> ("
+ 31 "        HashMap<String, HashSet<modifications::Tree>>,"
+ 32 "        HashMap<String, HashMap<String, Vec<modifications::Blob>>>,"
+ 33 "    ) {"
+ 34 "        // tree_map: map<parent_directory, Vec<changes>>"
+ 35 "        // blob_map: map<parent_directory, map<file_name, Vec<changes>>>"
+ 36 ""
+ 37 "        let mut tree_map = HashMap::new();"
+ 38 "        for tree_modification in &self.trees {"
+ 39 "            let path = match tree_modification {"
+ 40 "                modifications::Tree::CreateTree(path, _)"
+ 41 "                | modifications::Tree::DeleteTree(path, _)"
+ 42 "                | modifications::Tree::CreateBlob(path, _)"
+ 43 "                | modifications::Tree::DeleteBlob(path, _) => path.clone(),"
+ 44 "            };"
+ 45 ""
+ 46 "            assert_eq!(path, tree_modification.extract_data().0);"
+ 47 ""
+ 48 "            tree_map"
+ 49 "                .entry(path)"
+ 50 "                .or_insert(HashSet::new())"
+ 51 "                .insert(tree_modification.clone());"
+ 52 "        }"
+ 53 ""
+ 54 "        let mut blob_map = HashMap::new();"
+ 55 "        for blob_modification in &self.blobs {"
+ 56 "            let (parent_directory, file_name) = match blob_modification {"
+ 57 "                modifications::Blob::Create(path, name, _, _) => (path.clone(), name.clone()),"
+ 58 "                modifications::Blob::Delete(path, name, _, _) => (path.clone(), name.clone()),"
+ 59 "            };"
+ 60 ""
+ 61 "            assert_eq!("
+ 62 "                (parent_directory.clone(), file_name.clone()),"
+ 63 "                blob_modification.extract_path()"
+ 64 "            );"
+ 65 "            blob_map"
+ 66 "                .entry(parent_directory)"
+ 67 "                .or_insert(HashMap::new())"
+ 68 "                .entry(file_name)"
+ 69 "                .or_insert(vec![])"
+ 70 "                .push(blob_modification.clone());"
+ 71 "        }"
+ 72 ""
+ 73 "        (tree_map, blob_map)"
+ 74 "    }"
+ 75 "}"
| .%2Fsrc%2Fcore%2Fobjects%2Fmodifications%2Fchange serialisation.rs
+ 0 "use std::collections::HashMap;"
+ 1 ""
+ 2 "use crate::core::modifications;"
+ 3 ""
+ 4 "use super::Change;"
+ 5 ""
+ 6 "impl Change {"
+ 7 "    pub fn serialise_changes(&self) -> String {"
+ 8 "        // + D . src"
+ 9 "        // + F .%2Fsrc utils.rs"
+ 10 "        // + F .%2Fsrc branch.rs"
+ 11 "        // ="
+ 12 "        // | .%2Fsrc content.rs"
+ 13 "        // + 0 \"use std::{collections::{HashMap, HashSet}, fs, path::{Path, PathBuf}, sync::{Arc, Mutex}};\""
+ 14 "        // + 1 \"\""
+ 15 ""
+ 16 "        // final result string"
+ 17 "        let mut result: Vec<String> = vec![];"
+ 18 ""
+ 19 "        for tree in &self.trees {"
+ 20 "            result.push(tree.serialise());"
+ 21 "        }"
+ 22 ""
+ 23 "        result.push(\"=\".to_string()); // container and blob section separator"
+ 24 ""
+ 25 "        let mut blob_sections = HashMap::new();"
+ 26 "        for blob in &self.blobs {"
+ 27 "            blob_sections"
+ 28 "                .entry(blob.extract_path())"
+ 29 "                .or_insert(vec![])"
+ 30 "                .push(blob.clone());"
+ 31 "        }"
+ 32 ""
+ 33 "        let mut keys = blob_sections"
+ 34 "            .iter()"
+ 35 "            .map(|x| x.0.clone())"
+ 36 "            .collect::<Vec<(String, String)>>();"
+ 37 ""
+ 38 "        keys.sort();"
+ 39 ""
+ 40 "        for (path, name) in keys {"
+ 41 "            let modifications = blob_sections.get(&(path.clone(), name.clone())).unwrap();"
+ 42 "            result.push(format!("
+ 43 "                \"| {} {}\","
+ 44 "                urlencoding::encode(&path).to_string(),"
+ 45 "                urlencoding::encode(&name).to_string()"
+ 46 "            ));"
+ 47 "            for blob in modifications {"
+ 48 "                result.push(blob.extract_change());"
+ 49 "            }"
+ 50 "        }"
+ 51 ""
+ 52 "        result.join(\"\\n\")"
+ 53 "    }"
+ 54 ""
+ 55 "    pub fn deserialise_changes(s: String) -> Option<Change> {"
+ 56 "        // + D . src"
+ 57 "        // + F .%2Fsrc utils.rs"
+ 58 "        // + F .%2Fsrc branch.rs"
+ 59 "        // ="
+ 60 "        // | .%2Fsrc content.rs"
+ 61 "        // + 0 \"use std::{collections::{HashMap, HashSet}, fs, path::{Path, PathBuf}, sync::{Arc, Mutex}};\""
+ 62 "        // + 1 \"\""
+ 63 ""
+ 64 "        let lines = s"
+ 65 "            .split(\"\\n\")"
+ 66 "            .map(|x| x.to_string())"
+ 67 "            .collect::<Vec<String>>();"
+ 68 ""
+ 69 "        let mut result = Change::empty();"
+ 70 "        let mut tree_section = true;"
+ 71 ""
+ 72 "        let mut previous_blob = None;"
+ 73 "        for l in lines {"
+ 74 "            if tree_section && (l == \"=\") {"
+ 75 "                tree_section = false;"
+ 76 "                continue;"
+ 77 "            }"
+ 78 "            let content = l.split(\" \").collect::<Vec<&str>>();"
+ 79 ""
+ 80 "            if tree_section {"
+ 81 "                let [species, container, parent, name] = *content.as_slice() else {"
+ 82 "                    return None;"
+ 83 "                };"
+ 84 ""
+ 85 "                result.trees.push(match (species, container) {"
+ 86 "                    (\"+\", \"D\") => modifications::Tree::CreateTree("
+ 87 "                        urlencoding::decode(parent).unwrap().to_string(),"
+ 88 "                        urlencoding::decode(name).unwrap().to_string(),"
+ 89 "                    ),"
+ 90 "                    (\"-\", \"D\") => modifications::Tree::DeleteTree("
+ 91 "                        urlencoding::decode(parent).unwrap().to_string(),"
+ 92 "                        urlencoding::decode(name).unwrap().to_string(),"
+ 93 "                    ),"
+ 94 "                    (\"+\", \"F\") => modifications::Tree::CreateBlob("
+ 95 "                        urlencoding::decode(parent).unwrap().to_string(),"
+ 96 "                        urlencoding::decode(name).unwrap().to_string(),"
+ 97 "                    ),"
+ 98 "                    (\"-\", \"F\") => modifications::Tree::DeleteBlob("
+ 99 "                        urlencoding::decode(parent).unwrap().to_string(),"
+ 100 "                        urlencoding::decode(name).unwrap().to_string(),"
+ 101 "                    ),"
+ 102 "                    _ => {"
+ 103 "                        println!(\"invalid tree\");"
+ 104 "                        return None;"
+ 105 "                    }"
+ 106 "                });"
+ 107 "            } else {"
+ 108 "                if content[0] == \"|\" {"
+ 109 "                    // | .%2Fsrc content.rs"
+ 110 "                    let [_, parent, name] = *content.as_slice() else {"
+ 111 "                        println!(\"invalid blob header\");"
+ 112 "                        return None;"
+ 113 "                    };"
+ 114 ""
+ 115 "                    previous_blob = Some((parent.to_string(), name.to_string()));"
+ 116 "                } else {"
+ 117 "                    // + 0 \"use std::{collections::{HashMap, HashSet}, fs, path::{Path, PathBuf}, sync::{Arc, Mutex}};\""
+ 118 "                    if content.len() < 2 {"
+ 119 "                        println!(\"invalid change line\");"
+ 120 "                        return None;"
+ 121 "                    }"
+ 122 ""
+ 123 "                    let species = content[0];"
+ 124 "                    let line = match content[1].parse::<usize>() {"
+ 125 "                        Ok(i) => i,"
+ 126 "                        _ => {"
+ 127 "                            println!(\"invalid line index\");"
+ 128 "                            return None;"
+ 129 "                        }"
+ 130 "                    };"
+ 131 ""
+ 132 "                    match &previous_blob {"
+ 133 "                        Some((p, n)) => {"
+ 134 "                            let decoded_path = urlencoding::decode(p).unwrap().to_string();"
+ 135 "                            let decoded_name = urlencoding::decode(n).unwrap().to_string();"
+ 136 "                            let s = unescape::unescape(&content[2..].join(\" \")).unwrap();"
+ 137 "                            let content_text = s[1..s.len() - 1].to_string();"
+ 138 ""
+ 139 "                            match species {"
+ 140 "                                \"+\" => {"
+ 141 "                                    result.blobs.push(modifications::Blob::Create("
+ 142 "                                        decoded_path,"
+ 143 "                                        decoded_name,"
+ 144 "                                        line,"
+ 145 "                                        content_text,"
+ 146 "                                    ));"
+ 147 "                                }"
+ 148 "                                \"-\" => {"
+ 149 "                                    result.blobs.push(modifications::Blob::Delete("
+ 150 "                                        decoded_path,"
+ 151 "                                        decoded_name,"
+ 152 "                                        line,"
+ 153 "                                        content_text,"
+ 154 "                                    ));"
+ 155 "                                }"
+ 156 "                                _ => {"
+ 157 "                                    return None;"
+ 158 "                                }"
+ 159 "                            }"
+ 160 "                        }"
+ 161 "                        None => {"
+ 162 "                            return None;"
+ 163 "                        }"
+ 164 "                    }"
+ 165 "                }"
+ 166 "            }"
+ 167 "        }"
+ 168 ""
+ 169 "        Some(result)"
+ 170 "    }"
+ 171 "}"